{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d139c-378d-4fe2-b7e9-06c9aac0d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Visualization of data distribution\n",
    "# Uncomment to create exploratory visualizations\n",
    "\n",
    "# def explore_data_distributions(df):\n",
    "#     \"\"\"Create exploratory visualizations of key variables.\"\"\"\n",
    "#     # Set up a figure with subplots\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "#     \n",
    "#     # Plot IT and V2 difference distributions\n",
    "#     sns.histplot(df['it_sim_dis_diff'], kde=True, ax=axes[0, 0])\n",
    "#     axes[0, 0].set_title('IT Similarity Difference Distribution')\n",
    "#     \n",
    "#     sns.histplot(df['v2_sim_dis_diff'], kde=True, ax=axes[0, 1])\n",
    "#     axes[0, 1].set_title('V2 Similarity Difference Distribution')\n",
    "#     \n",
    "#     # Plot accuracy by condition\n",
    "#     sns.boxplot(x='Retrocue Reliability', y='Accuracy', data=df, ax=axes[1, 0])\n",
    "#     axes[1, 0].set_title('Accuracy by Retrocue Reliability')\n",
    "#     \n",
    "#     sns.boxplot(x='Tested Item', y='Accuracy', data=df, ax=axes[1, 1])\n",
    "#     axes[1, 1].set_title('Accuracy by Tested Item')\n",
    "#     \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "# \n",
    "# # Create exploratory visualizations\n",
    "# explore_data_distributions(merged_df)\n",
    "\n",
    "def summarize_final_dataset(df):\n",
    "    \"\"\"Print a summary of the final dataset.\"\"\"\n",
    "    print(\"\\n=== Final Dataset Summary ===\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique participants: {len(df['participant'].unique())}\")\n",
    "    print(f\"Task phases present: {df['taskPhase'].unique()}\")\n",
    "    print(f\"Pilot 1 participants: {len(df[df['pilot_number'] == 1]['participant'].unique())}\")\n",
    "    print(f\"Pilot 2 participants: {len(df[df['pilot_number'] == 2]['participant'].unique())}\")\n",
    "    print(f\"Columns in final dataset: {len(df.columns)}\")\n",
    "    print(\"Data types in final dataset:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    print(\"\\nMemory usage:\", df.memory_usage().sum() / 1e6, \"MB\")\n",
    "\n",
    "# Summarize the final dataset\n",
    "summarize_final_dataset(merged_df)\n",
    "\n",
    "# Save the final processed dataframe\n",
    "merged_df.to_csv('pilot_TOTAL_merged_questionnaires.csv', index=False)\n",
    "print(\"Saved merged dataset to pilot_TOTAL_merged_questionnaires.csv\")\n",
    "\n",
    "def merge_with_questionnaires(df, questionnaire_file='wm_questionnaires_preprocessed.csv'):\n",
    "    \"\"\"Merge behavioral data with questionnaire data.\"\"\"\n",
    "    # Load questionnaire data\n",
    "    df_quest = pd.read_csv(questionnaire_file)\n",
    "    print(f\"Loaded questionnaire data with {len(df_quest)} rows\")\n",
    "    \n",
    "    # Check for correspondence between datasets\n",
    "    unique_subject_ids = df_quest['subject_id'].unique()\n",
    "    unique_participants = df['participant'].unique()\n",
    "    \n",
    "    print(f\"Number of unique subject_ids in questionnaire data: {len(unique_subject_ids)}\")\n",
    "    print(f\"Number of unique participants in behavioral data: {len(unique_participants)}\")\n",
    "    \n",
    "    common_participants = np.intersect1d(unique_participants, unique_subject_ids)\n",
    "    print(f\"Number of participants in both datasets: {len(common_participants)}\")\n",
    "    print(f\"Percentage of behavioral participants in questionnaire data: {len(common_participants)/len(unique_participants)*100:.2f}%\")\n",
    "    \n",
    "    # Merge dataframes\n",
    "    merged_df = df.merge(\n",
    "        df_quest,\n",
    "        left_on='participant',  # Column in df\n",
    "        right_on='subject_id',  # Column in df_quest\n",
    "        how='left'              # Keep all rows from df\n",
    "    )\n",
    "    \n",
    "    # Check for missing questionnaire data\n",
    "    na_count = merged_df['vviq_sum'].isna().sum()\n",
    "    print(f\"Number of rows with missing vviq_sum: {na_count}\")\n",
    "    \n",
    "    # Remove rows with missing questionnaire data\n",
    "    merged_df = merged_df.dropna(subset=['vviq_sum'])\n",
    "    merged_df = merged_df.dropna(subset=['osivq_visual_mean'])\n",
    "    \n",
    "    print(f\"Merged dataframe has {len(merged_df)} rows after removing missing questionnaire data\")\n",
    "    print(f\"Merged dataframe has {len(merged_df['participant'].unique())} unique participants\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Merge with questionnaire data\n",
    "merged_df = merge_with_questionnaires(df)\n",
    "\n",
    "def add_memorability_features(df):\n",
    "    \"\"\"Add memorability features from separate prediction files for each pilot.\"\"\"\n",
    "    # Mark each row with its pilot number\n",
    "    df['pilot_number'] = np.where(df['date'] <= '2024-10-31', 1, 2)\n",
    "    \n",
    "    # Split dataframe by pilot\n",
    "    df_1 = df[df['pilot_number'] == 1].copy()\n",
    "    df_2 = df[df['pilot_number'] == 2].copy()\n",
    "    \n",
    "    # Print participant counts\n",
    "    print(f\"Pilot 1 participants: {len(set(df_1['participant']))}\")\n",
    "    print(f\"Pilot 2 participants: {len(set(df_2['participant']))}\")\n",
    "    \n",
    "    def add_memorability_to_df(df_subset, mem_file):\n",
    "        \"\"\"Add memorability features to a dataframe subset.\"\"\"\n",
    "        # Load memorability data\n",
    "        df_memorability = pd.read_csv(mem_file)\n",
    "        df_memorability['filename'] = df_memorability['filename'].str.replace('new_stimuli', 'stimuli')\n",
    "        memorability_dict = dict(zip(df_memorability['filename'], df_memorability['predictions']))\n",
    "        \n",
    "        # Create memorability columns\n",
    "        df_subset['tested_memorability_resmem'] = np.where(\n",
    "            df_subset['test_item'] == 'img1',\n",
    "            df_subset['img1'].astype(str).map(memorability_dict),\n",
    "            df_subset['img2'].astype(str).map(memorability_dict)\n",
    "        )\n",
    "        \n",
    "        df_subset['untested_memorability_resmem'] = np.where(\n",
    "            df_subset['test_item'] == 'img1',\n",
    "            df_subset['img2'].astype(str).map(memorability_dict),\n",
    "            df_subset['img1'].astype(str).map(memorability_dict)\n",
    "        )\n",
    "        \n",
    "        df_subset['attended_memorability_resmem'] = np.where(\n",
    "            df_subset['attend'] == 'img1',\n",
    "            df_subset['img1'].astype(str).map(memorability_dict),\n",
    "            df_subset['img2'].astype(str).map(memorability_dict)\n",
    "        )\n",
    "        \n",
    "        df_subset['unattended_memorability_resmem'] = np.where(\n",
    "            df_subset['attend'] == 'img1',\n",
    "            df_subset['img2'].astype(str).map(memorability_dict),\n",
    "            df_subset['img1'].astype(str).map(memorability_dict)\n",
    "        )\n",
    "        \n",
    "        df_subset['distractor_memorability'] = df_subset['ping_img'].astype(str).map(memorability_dict)\n",
    "        df_subset['tested_memorability_resmem_z'] = scaler.fit_transform(df_subset[['tested_memorability_resmem']])\n",
    "        \n",
    "        return df_subset\n",
    "    \n",
    "    # Add memorability features to each pilot subset\n",
    "    df_1 = add_memorability_to_df(df_1, 'predictions_pilot5.csv')\n",
    "    df_2 = add_memorability_to_df(df_2, 'predictions_pilot6.csv')\n",
    "    \n",
    "    # Combine the datasets\n",
    "    combined_df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Add memorability features\n",
    "df = add_memorability_features(df)\n",
    "print(f\"Added memorability features. Total columns: {len(df.columns)}\")\n",
    "\n",
    "def handle_participants_over_300(df):\n",
    "    \"\"\"Handle participants with more than 300 entries by keeping only the first 300.\"\"\"\n",
    "    participant_counts = df['participant'].value_counts()\n",
    "    participants_over_300 = participant_counts[participant_counts > 300]\n",
    "    \n",
    "    if not participants_over_300.empty:\n",
    "        print(f\"Found {len(participants_over_300)} participants appearing in multiple files:\")\n",
    "        print(participants_over_300)\n",
    "        \n",
    "        rows_to_keep = []\n",
    "        for participant in participants_over_300.index:\n",
    "            files = df[df['participant'] == participant]['filename'].unique()\n",
    "            print(f\"\\nParticipant {participant} appears in {len(files)} files:\")\n",
    "            \n",
    "            for file in files:\n",
    "                count = df[(df['participant'] == participant) & (df['filename'] == file)].shape[0]\n",
    "                print(f\"  - {file}: {count} times\")\n",
    "                \n",
    "            participant_data = df[df['participant'] == participant]\n",
    "            participant_data_limited = participant_data.head(300)\n",
    "            rows_to_keep.extend(participant_data_limited.index.tolist())\n",
    "            \n",
    "            print(f\"Keeping first 300 rows for participant {participant}, dropping {len(participant_data) - 300} rows\")\n",
    "    \n",
    "        # Also keep rows for participants with 300 or fewer entries\n",
    "        for participant in participant_counts[participant_counts <= 300].index:\n",
    "            participant_data = df[df['participant'] == participant]\n",
    "            rows_to_keep.extend(participant_data.index.tolist())\n",
    "            \n",
    "        # Create the cleaned dataframe by selecting only the rows to keep\n",
    "        df_cleaned = df.loc[rows_to_keep]\n",
    "        return df_cleaned\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Handle participants with more than 300 entries\n",
    "df = handle_participants_over_300(df)\n",
    "print(f\"After handling participants with >300 entries: {len(df)} rows\")\n",
    "\n",
    "# Filter to main task only\n",
    "df = df[df['taskPhase'] == 'mainTask']\n",
    "print(f\"After filtering to main task: {len(df)} rows\")\n",
    "\n",
    "def clean_dataframe_from_nonresp(df):\n",
    "    \"\"\"Filter out rows with missing mouse response data.\"\"\"\n",
    "    # Define a filter to check if mouse data exists\n",
    "    def filter_ranges(numbers):\n",
    "        if numbers is None:\n",
    "            return False\n",
    "        return all(num for num in numbers)\n",
    "    \n",
    "    # Apply filters for mouse time and click\n",
    "    filtered_df = df[df['processed_mouse.time'].apply(filter_ranges)]\n",
    "    filtered_df = filtered_df[filtered_df['processed_mouse.click'].apply(filter_ranges)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def df_with_threshold(df, numeric_columns, threshold=0.4):\n",
    "    \"\"\"Filter to include only participants with accuracy above threshold.\"\"\"\n",
    "    # Calculate mean accuracy by participant\n",
    "    sub_df = df[numeric_columns].groupby('participant').mean().reset_index()\n",
    "    \n",
    "    # Find participants above threshold\n",
    "    above_threshold_subs = sub_df.loc[sub_df['resp_correct'] >= threshold, 'participant']\n",
    "    \n",
    "    # Filter dataframe\n",
    "    df_filtered = df[df['participant'].isin(above_threshold_subs)]\n",
    "    \n",
    "    # Remove unit variance in accuracy\n",
    "    df_filtered = remove_unit_variance(df_filtered, 'resp_correct', 'participant')\n",
    "    df_filtered['Accuracy'] = df_filtered['resp_correct_within']\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply cleaning operations\n",
    "df_nonresp = clean_dataframe_from_nonresp(df)\n",
    "print(f\"After filtering non-responses: {len(df_nonresp)} rows\")\n",
    "\n",
    "df = df_with_threshold(df, numeric_columns, 0.4)\n",
    "print(f\"After accuracy threshold filtering: {len(df)} rows with {df['participant'].nunique()} participants\")\n",
    "\n",
    "def process_mouse_data(df):\n",
    "    \"\"\"Process mouse data to extract time and click information.\"\"\"\n",
    "    df_mouse = df.copy()\n",
    "    \n",
    "    # Process mouse time and click data\n",
    "    df_mouse['processed_mouse.time'] = df['mouse.time'].apply(process_values_time)\n",
    "    df_mouse['processed_mouse.click'] = df['mouse.clicked_name'].apply(process_values_click)\n",
    "    \n",
    "    # Extract length, first and last values\n",
    "    df_mouse['mouse.time_length'] = df_mouse['processed_mouse.time'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else 0\n",
    "    )\n",
    "    df_mouse['mouse.time_first'] = df_mouse['processed_mouse.time'].apply(\n",
    "        lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    df_mouse['mouse.time_last'] = df_mouse['processed_mouse.time'].apply(\n",
    "        lambda x: x[-1] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    df_mouse['mouse.clicked_name_first'] = df_mouse['processed_mouse.click'].apply(\n",
    "        lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    df_mouse['mouse.clicked_name_last'] = df_mouse['processed_mouse.click'].apply(\n",
    "        lambda x: x[-1] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_mouse\n",
    "\n",
    "# Process mouse data\n",
    "df = process_mouse_data(df)\n",
    "print(f\"Processed mouse data. Total columns: {len(df.columns)}\")\n",
    "\n",
    "# Set reaction time for analysis\n",
    "df['rt'] = df['mouse.time_last']\n",
    "df = remove_unit_variance(df, 'mouse.time_last', 'participant')\n",
    "df['analysis_rt'] = df['mouse.time_last_within']\n",
    "\n",
    "print(\"Added reaction time analysis variables\")\n",
    "\n",
    "# Create positive/negative indicators and interaction terms\n",
    "pos_neg_columns = {\n",
    "    'it_pos_neg': np.where(df['it_sim_dis_diff_test'] <= 0, -1, 1),\n",
    "    'v2_pos_neg': np.where(df['v2_sim_dis_diff_test'] <= 0, -1, 1),\n",
    "    'it_pos_neg_abs': np.where(df['it_sim_dis_test'] <= 0, -1, 1),\n",
    "    'v2_pos_neg_abs': np.where(df['v2_sim_dis_test'] <= 0, -1, 1)\n",
    "}\n",
    "\n",
    "# Add positive/negative columns\n",
    "for col_name, values in pos_neg_columns.items():\n",
    "    df[col_name] = values\n",
    "    df[col_name + '_z'] = scaler.fit_transform(df[[col_name]])\n",
    "\n",
    "# Create interaction terms\n",
    "interaction_terms = {\n",
    "    'it_int_rel': df['it_pos_neg'] * df['it_sim_dis_diff_test_sign_z'],\n",
    "    'v2_int_rel': df['v2_pos_neg'] * df['v2_sim_dis_diff_test_sign_z'],\n",
    "    'it_int_abs': df['it_pos_neg_abs'] * df['it_sim_dis_test_sign_z'],\n",
    "    'v2_int_abs': df['v2_pos_neg_abs'] * df['v2_sim_dis_test_sign_z'],\n",
    "    'it_int_rel_sq': df['it_pos_neg'] * df['it_sim_dis_diff_test_sign_sq_z'],\n",
    "    'v2_int_rel_sq': df['v2_pos_neg'] * df['v2_sim_dis_diff_test_sign_sq_z'],\n",
    "    'it_int_abs_sq': df['it_pos_neg_abs'] * df['it_sim_dis_test_sign_sq_z'],\n",
    "    'v2_int_abs_sq': df['v2_pos_neg_abs'] * df['v2_sim_dis_test_sign_sq_z']\n",
    "}\n",
    "\n",
    "# Add interaction terms at once\n",
    "for col_name, values in interaction_terms.items():\n",
    "    df[col_name] = values\n",
    "\n",
    "print(f\"Added interaction terms. Total columns: {len(df.columns)}\")\n",
    "\n",
    "def flip_z_sq_z(df, column_name):\n",
    "    \"\"\"\n",
    "    Create sign-preserving transformations for a column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    column_name : str\n",
    "        Column to transform\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with new columns\n",
    "    \"\"\"\n",
    "    df_flip = df.copy()\n",
    "    \n",
    "    # Dictionary to store all new columns\n",
    "    new_columns = {}\n",
    "    \n",
    "    # Create sign-preserved value\n",
    "    new_columns[column_name + '_sign'] = np.where(df[column_name] <= 0, -1, 1) * df[column_name]\n",
    "    \n",
    "    # Z-score the sign-preserved value\n",
    "    new_columns[column_name + '_sign_z'] = scaler.fit_transform(\n",
    "        pd.DataFrame(new_columns[column_name + '_sign'])\n",
    "    ).flatten()\n",
    "    \n",
    "    # Square and z-score\n",
    "    new_columns[column_name + '_sign_sq'] = new_columns[column_name + '_sign_z'] ** 2\n",
    "    new_columns[column_name + '_sign_sq_z'] = scaler.fit_transform(\n",
    "        pd.DataFrame(new_columns[column_name + '_sign_sq'])\n",
    "    ).flatten()\n",
    "    \n",
    "    # Add all new columns at once\n",
    "    for col_name, values in new_columns.items():\n",
    "        df_flip[col_name] = values\n",
    "    \n",
    "    return df_flip\n",
    "\n",
    "# Apply sign-preserving transformations\n",
    "for column in ['it_sim_dis_diff_test', 'v2_sim_dis_diff_test', 'it_sim_dis_test', 'v2_sim_dis_test']:\n",
    "    df = flip_z_sq_z(df, column)\n",
    "\n",
    "print(f\"Added sign-preserving transformations. Total columns: {len(df.columns)}\"){\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"intro-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Optimized Data Processing Notebook\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook processes experimental data, calculates various metrics, and merges with questionnaire data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"import-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import os.path as op\\n\",\n",
    "    \"import glob\\n\",\n",
    "    \"from datetime import timedelta\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"setup-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Configuration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"setup-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Setup directory and visualization style\\n\",\n",
    "    \"home_dir = op.abspath('./')\\n\",\n",
    "    \"data_files = glob.glob(op.join(home_dir, 'data', '*.csv'))\\n\",\n",
    "    \"sns.set_context('talk')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameters for similarity categorization\\n\",\n",
    "    \"column_params = {\\n\",\n",
    "    \"    'v2': {'n_cats': 5, 'labels': ['Least Similar', '', '  ', '   ', 'Most Similar']},\\n\",\n",
    "    \"    'it': {'n_cats': 5, 'labels': ['Least Similar', '', '  ', '   ', 'Most Similar']}\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a global StandardScaler\\n\",\n",
    "    \"scaler = StandardScaler()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"helper-functions-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Helper Functions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"helper-functions-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def remove_unit_variance(df, col, unit, group=None, suffix=\\\"_within\\\"):\\n\",\n",
    "    \"    \\\"\\\"\\\"Remove variance between sampling units.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    This is useful for plotting repeated-measures data using within-unit\\n\",\n",
    "    \"    error bars.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Parameters\\n\",\n",
    "    \"    ----------\\n\",\n",
    "    \"    df : DataFrame\\n\",\n",
    "    \"        Input data. Will have a new column added.\\n\",\n",
    "    \"    col : column name\\n\",\n",
    "    \"        Column in dataframe with quantitative measure to modify.\\n\",\n",
    "    \"    unit : column name\\n\",\n",
    "    \"        Column in dataframe defining sampling units (e.g., subjects).\\n\",\n",
    "    \"    group : column name(s), optional\\n\",\n",
    "    \"        Columns defining groups to remove unit variance within.\\n\",\n",
    "    \"    suffix : string, optional\\n\",\n",
    "    \"        Suffix appended to ``col`` name to create new column.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns\\n\",\n",
    "    \"    -------\\n\",\n",
    "    \"    df : DataFrame\\n\",\n",
    "    \"        Returns modified dataframe.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    new_col = col + suffix\\n\",\n",
    "    \"    df_copy = df.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def demean(x):\\n\",\n",
    "    \"        return x - x.mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if group is None:\\n\",\n",
    "    \"        new = df_copy.groupby(unit)[col].transform(demean)\\n\",\n",
    "    \"        new += df_copy[col].mean()\\n\",\n",
    "    \"        df_copy[new_col] = new\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        df_copy[new_col] = np.nan\\n\",\n",
    "    \"        for level, df_level in df_copy.groupby(group):\\n\",\n",
    "    \"            new = df_level.groupby(unit)[col].transform(demean)\\n\",\n",
    "    \"            new += df_level[col].mean()\\n\",\n",
    "    \"            df_copy.loc[new.index, new_col] = new\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return df_copy\\n\",\n",
    "    \"\\n\",\n",
    "    \"def parse_dates(series):\\n\",\n",
    "    \"    \\\"\\\"\\\"Parse date strings, handling special cases like '24h'.\\\"\\\"\\\"\\n\",\n",
    "    \"    date_str = series.iloc[0]\\n\",\n",
    "    \"    if \\\"24h\\\" in date_str:\\n\",\n",
    "    \"        corrected_date_str = date_str.replace(\\\"24h\\\", \\\"00h\\\")\\n\",\n",
    "    \"        dt = pd.to_datetime(corrected_date_str, format='%Y-%m-%d_%Hh%M.%S.%f')\\n\",\n",
    "    \"        dt += timedelta(days=1)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        dt = pd.to_datetime(date_str, format='%Y-%m-%d_%Hh%M.%S.%f')\\n\",\n",
    "    \"    return dt\\n\",\n",
    "    \"\\n\",\n",
    "    \"def process_values_time(value):\\n\",\n",
    "    \"    \\\"\\\"\\\"Process mouse time values from string to list of numbers.\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\\n\",\n",
    "    \"            cleaned = value.strip('[]').split(',')\\n\",\n",
    "    \"            if cleaned == ['']:  # Check if the list after stripping is empty\\n\",\n",
    "    \"                return None\\n\",\n",
    "    \"            numbers = [float(num) for num in cleaned if num.strip()]\\n\",\n",
    "    \"            return numbers\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    except ValueError:\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"def process_values_click(value):\\n\",\n",
    "    \"    \\\"\\\"\\\"Process mouse click values from string to list of items.\\\"\\\"\\\"\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\\n\",\n",
    "    \"            cleaned = value.strip('[]').split(',')\\n\",\n",
    "    \"            if cleaned == ['']:  # Check if the list after stripping is empty\\n\",\n",
    "    \"                return None\\n\",\n",
    "    \"            return [item for item in cleaned if item.strip()]\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    except ValueError:\\n\",\n",
    "    \"        return None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"data-loading-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Data Loading and Initial Processing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"data-loading-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def df_creation(data_files, start_date, end_date):\\n\",\n",
    "    \"    \\\"\\\"\\\"Create a dataframe from the data files within a date range.\\\"\\\"\\\"\\n\",\n",
    "    \"    processed_dfs = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Loop through files and try to read them\\n\",\n",
    "    \"    for file_path in data_files:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            temp_df = pd.read_csv(file_path)\\n\",\n",
    "    \"            temp_df['filename'] = file_path\\n\",\n",
    "    \"            processed_dfs.append(temp_df)\\n\",\n",
    "    \"        except Exception:\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Process the loaded dataframes\\n\",\n",
    "    \"    if processed_dfs:\\n\",\n",
    "    \"        # Concatenate all dataframes\\n\",\n",
    "    \"        df = pd.concat(processed_dfs, ignore_index=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert date column and filter by date range\\n\",\n",
    "    \"        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d_%Hh%M.%S.%f', errors='coerce')\\n\",\n",
    "    \"        df.dropna(subset=['date'], inplace=True)\\n\",\n",
    "    \"        df = df[(df['date'] >= pd.to_datetime(start_date)) & (df['date'] <= pd.to_datetime(end_date))]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Filter rows with non-null V2_diff values\\n\",\n",
    "    \"        df = df.loc[df['V2_diff'].notnull()].reset_index(drop=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert reliability to float and create Retrocue Reliability\\n\",\n",
    "    \"        df['reliability'] = df['reliability'].astype(float)\\n\",\n",
    "    \"        df['Retrocue Reliability'] = np.where(df['reliability'] > 0.75, 'high', 'low')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get numeric columns for later use\\n\",\n",
    "    \"        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return df, numeric_columns\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Return empty DataFrame if no files were processed\\n\",\n",
    "    \"        return pd.DataFrame(), []\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load data from both pilot periods\\n\",\n",
    "    \"df1, numeric_columns1 = df_creation(data_files, '2024-10-08', '2024-10-30')  # pilot5\\n\",\n",
    "    \"df2, numeric_columns2 = df_creation(data_files, '2024-11-22', '2025-01-30')  # pilot6\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Concatenate the data from both pilots\\n\",\n",
    "    \"df = pd.concat([df1, df2], axis=0)\\n\",\n",
    "    \"numeric_columns = numeric_columns1  # Use columns from first dataset\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print basic information about the loaded data\\n\",\n",
    "    \"print(f\\\"Loaded {len(df)} rows of data\\\")\\n\",\n",
    "    \"print(f\\\"Data from pilot 1: {len(df1)} rows\\\")\\n\",\n",
    "    \"print(f\\\"Data from pilot 2: {len(df2)} rows\\\")\\n\",\n",
    "    \"print(f\\\"Total unique participants: {df['participant'].nunique()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"add-trial-info-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Add Trial Information\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"add-trial-info-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Add trial number and batch information\\n\",\n",
    "    \"def add_trial_info(df, participant_col, trials_per_batch=30):\\n\",\n",
    "    \"    \\\"\\\"\\\"Add trial number and batch information to the dataframe.\\\"\\\"\\\"\\n\",\n",
    "    \"    df['Trial_Number'] = df.groupby(participant_col).cumcount() + 1\\n\",\n",
    "    \"    df['Trial_Batch'] = ((df['Trial_Number'] - 1) // trials_per_batch) + 1\\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"df = add_trial_info(df, participant_col='participant')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add condition batch information\\n\",\n",
    "    \"# Extract batch info from trial 182\\n\",\n",
    "    \"df_trial_182 = df[df['trial'] == 182][['participant', 'trial', 'cond_file', 'root', 'IT_diff']].copy()\\n\",\n",
    "    \"df_trial_182['conditions_batch'] = df_trial_182.groupby(['cond_file', 'root', 'IT_diff']).ngroup() + 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Count participants per batch\\n\",\n",
    "    \"batch_info = df_trial_182.groupby('conditions_batch').agg(\\n\",\n",
    "    \"    participants_count=('participant', 'nunique'),\\n\",\n",
    "    \"    participants_list=('participant', 'unique')\\n\",\n",
    "    \").reset_index()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Merge batch info back to the main dataframe\\n\",\n",
    "    \"df = df.merge(\\n\",\n",
    "    \"    df_trial_182[['participant', 'conditions_batch', 'participants_count', 'participants_list']],\\n\",\n",
    "    \"    on='participant',\\n\",\n",
    "    \"    how='left'\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Added trial numbers and batch information. Max trial number: {df['Trial_Number'].max()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"signal-diff-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Calculate Signal Differences and Feature Engineering\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"signal-diff-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def calculate_differences(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Calculate differences between conditions for IT and V2 signals.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Create a copy to avoid fragmentation warnings\\n\",\n",
    "    \"    df_diff = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create all new columns in a single dictionary\\n\",\n",
    "    \"    new_columns = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Attended and unattended conditions\\n\",\n",
    "    \"    new_columns['it_sim_dis_attend'] = np.where(df['attend'] == 'img1', df['IT_root_im1'], df['IT_root_im2'])\\n\",\n",
    "    \"    new_columns['v2_sim_dis_attend'] = np.where(df['attend'] == 'img1', df['V2_root_im1'], df['V2_root_im2'])\\n\",\n",
    "    \"    new_columns['it_sim_dis_test'] = np.where(df['test_item'] == 'img1', df['IT_root_im1'], df['IT_root_im2'])\\n\",\n",
    "    \"    new_columns['v2_sim_dis_test'] = np.where(df['test_item'] == 'img1', df['V2_root_im1'], df['V2_root_im2'])\\n\",\n",
    "    \"    new_columns['it_sim_dis_unattend'] = np.where(df['attend'] != 'img1', df['IT_root_im1'], df['IT_root_im2'])\\n\",\n",
    "    \"    new_columns['v2_sim_dis_unattend'] = np.where(df['attend'] != 'img1', df['V2_root_im1'], df['V2_root_im2'])\\n\",\n",
    "    \"    new_columns['it_sim_dis_untest'] = np.where(df['test_item'] != 'img1', df['IT_root_im1'], df['IT_root_im2'])\\n\",\n",
    "    \"    new_columns['v2_sim_dis_untest'] = np.where(df['test_item'] != 'img1', df['V2_root_im1'], df['V2_root_im2'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate differences\\n\",\n",
    "    \"    new_columns['it_sim_dis_diff'] = np.where(\\n\",\n",
    "    \"        df['attend'] == 'img1', \\n\",\n",
    "    \"        df['IT_root_im1'] - df['IT_root_im2'], \\n\",\n",
    "    \"        df['IT_root_im2'] - df['IT_root_im1']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    new_columns['v2_sim_dis_diff'] = np.where(\\n\",\n",
    "    \"        df['attend'] == 'img1', \\n\",\n",
    "    \"        df['V2_root_im1'] - df['V2_root_im2'], \\n\",\n",
    "    \"        df['V2_root_im2'] - df['V2_root_im1']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    new_columns['it_sim_dis_diff_test'] = np.where(\\n\",\n",
    "    \"        df['test_item'] == 'img1', \\n\",\n",
    "    \"        df['IT_root_im1'] - df['IT_root_im2'], \\n\",\n",
    "    \"        df['IT_root_im2'] - df['IT_root_im1']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    new_columns['v2_sim_dis_diff_test'] = np.where(\\n\",\n",
    "    \"        df['test_item'] == 'img1', \\n\",\n",
    "    \"        df['V2_root_im1'] - df['V2_root_im2'], \\n\",\n",
    "    \"        df['V2_root_im2'] - df['V2_root_im1']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Copy image similarity values\\n\",\n",
    "    \"    new_columns['it_im1_im2'] = df['IT_im1_im2']\\n\",\n",
    "    \"    new_columns['v2_im1_im2'] = df['V2_im1_im2']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add all new columns at once to avoid fragmentation\\n\",\n",
    "    \"    for col_name, values in new_columns.items():\\n\",\n",
    "    \"        df_diff[col_name] = values\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Determine convergence and preferences\\n\",\n",
    "    \"    df_diff['v2_converges'] = np.where(\\n\",\n",
    "    \"        (df_diff['it_sim_dis_diff'] > 0) & (df_diff['v2_sim_dis_diff'] > 0) | \\n\",\n",
    "    \"        (df_diff['it_sim_dis_diff'] < 0) & (df_diff['v2_sim_dis_diff'] < 0), \\n\",\n",
    "    \"        'V2/IT agree', 'V2/IT disagree'\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Determine preferences\\n\",\n",
    "    \"    df_diff['v2_prefers'] = np.where(df_diff['v2_sim_dis_diff'] > 0, 'Prioritized', 'Deprioritized')\\n\",\n",
    "    \"    df_diff['it_prefers'] = np.where(df_diff['it_sim_dis_diff'] > 0, 'Prioritized', 'Deprioritized')\\n\",\n",
    "    \"    df_diff['v2_prefers_test'] = np.where(df_diff['v2_sim_dis_diff_test'] > 0, 'Tested', 'Untested')\\n\",\n",
    "    \"    df_diff['it_prefers_test'] = np.where(df_diff['it_sim_dis_diff_test'] > 0, 'Tested', 'Untested')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add preference columns with better names\\n\",\n",
    "    \"    df_diff['Distractor V2 Similarity Preference Tested'] = df_diff['v2_prefers_test']\\n\",\n",
    "    \"    df_diff['Distractor IT Similarity Preference Tested'] = df_diff['it_prefers_test']\\n\",\n",
    "    \"    df_diff['Distractor V2 Similarity Preference'] = df_diff['v2_prefers']\\n\",\n",
    "    \"    df_diff['Distractor IT Similarity Preference'] = df_diff['it_prefers']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create binned versions of differences\\n\",\n",
    "    \"    df_diff['IT_diff_binned'] = pd.qcut(df_diff['it_sim_dis_diff'], 5, duplicates='drop')\\n\",\n",
    "    \"    df_diff['V2_diff_binned'] = pd.qcut(df_diff['v2_sim_dis_diff'], 5, duplicates='drop')\\n\",\n",
    "    \"    df_diff['IT_diff_binned_test'] = pd.qcut(df_diff['it_sim_dis_diff_test'], 5, duplicates='drop')\\n\",\n",
    "    \"    df_diff['V2_diff_binned_test'] = pd.qcut(df_diff['v2_sim_dis_diff_test'], 5, duplicates='drop')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df_diff\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply signal difference calculations\\n\",\n",
    "    \"df = calculate_differences(df)\\n\",\n",
    "    \"print(f\\\"Calculated signal differences and added {len(df.columns) - len(numeric_columns)} new columns\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"categorize-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Categorize Columns and Add User Interface Labels\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"categorize-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def categorize_columns(df, column_params):\\n\",\n",
    "    \"    \\\"\\\"\\\"Categorize specified columns into discrete categories based on quantiles.\\\"\\\"\\\"\\n\",\n",
    "    \"    df_cat = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create all categorized columns at once\\n\",\n",
    "    \"    cat_columns = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for label in [\\n\",\n",
    "    \"        'it_sim_dis_attend', 'v2_sim_dis_attend', 'it_sim_dis_unattend', 'v2_sim_dis_unattend',\\n\",\n",
    "    \"        'it_sim_dis_diff', 'v2_sim_dis_diff', 'it_im1_im2', 'v2_im1_im2',\\n\",\n",
    "    \"        'it_sim_dis_test', 'v2_sim_dis_test', 'it_sim_dis_untest', 'v2_sim_dis_untest',\\n\",\n",
    "    \"        'it_sim_dis_diff_test', 'v2_sim_dis_diff_test'\\n\",\n",
    "    \"    ]:\\n\",\n",
    "    \"        # Determine the column prefix\\n\",\n",
    "    \"        column_prefix = 'v2' if 'v2' in label else 'it'\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get parameters\\n\",\n",
    "    \"        n_cats = column_params[column_prefix]['n_cats']\\n\",\n",
    "    \"        labels = column_params[column_prefix]['labels']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create categorized column\\n\",\n",
    "    \"        cat_columns[label + '_cat'] = pd.qcut(\\n\",\n",
    "    \"            df_cat[label], \\n\",\n",
    "    \"            q=n_cats, \\n\",\n",
    "    \"            labels=labels, \\n\",\n",
    "    \"            duplicates='drop'\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add all categorized columns at once\\n\",\n",
    "    \"    for col_name, values in cat_columns.items():\\n\",\n",
    "    \"        df_cat[col_name] = values\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df_cat\\n\",\n",
    "    \"\\n\",\n",
    "    \"def df_column_addition(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Add user-friendly column names for plots and analyses.\\\"\\\"\\\"\\n\",\n",
    "    \"    df_add = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create a dictionary of all new columns\\n\",\n",
    "    \"    new_columns = {\\n\",\n",
    "    \"        'V2 Distractor Similarity\\\\nto Prioritized Item': df['v2_sim_dis_attend_cat'],\\n\",\n",
    "    \"        'IT Distractor Similarity\\\\nto Prioritized Item': df['it_sim_dis_attend_cat'],\\n\",\n",
    "    \"        'V2 Distractor Similarity\\\\nto Deprioritized Item': df['v2_sim_dis_unattend_cat'],\\n\",\n",
    "    \"        'IT Distractor Similarity\\\\nto Deprioritized Item': df['it_sim_dis_unattend_cat'],\\n\",\n",
    "    \"        'Prioritized - Deprioritized IT Distractor Similarity': df['it_sim_dis_diff_cat'],\\n\",\n",
    "    \"        'Prioritized - Deprioritized V2 Distractor Similarity': df['v2_sim_dis_diff_cat'],\\n\",\n",
    "    \"        'V2 Distractor Similarity\\\\nto Tested Item': df['v2_sim_dis_test_cat'],\\n\",\n",
    "    \"        'IT Distractor Similarity\\\\nto Tested Item': df['it_sim_dis_test_cat'],\\n\",\n",
    "    \"        'V2 Distractor Similarity\\\\nto Untested Item': df['v2_sim_dis_untest_cat'],\\n\",\n",
    "    \"        'IT Distractor Similarity\\\\nto Untested Item': df['it_sim_dis_untest_cat'],\\n\",\n",
    "    \"        'Tested - Untested IT Distractor Similarity': df['it_sim_dis_diff_test_cat'],\\n\",\n",
    "    \"        'Tested - Untested V2 Distractor Similarity': df['v2_sim_dis_diff_test_cat'],\\n\",\n",
    "    \"        'Prioritized - Deprioritized V2 Distractor Similarity Ranges': df['V2_diff_binned'],\\n\",\n",
    "    \"        'Prioritized - Deprioritized IT Distractor Similarity Ranges': df['IT_diff_binned'],\\n\",\n",
    "    \"        'Tested - Untested V2 Distractor Similarity Ranges': df['V2_diff_binned_test'],\\n\",\n",
    "    \"        'Tested - Untested IT Distractor Similarity Ranges': df['IT_diff_binned_test'],\\n\",\n",
    "    \"        'tested_item': df['Tested Item'],\\n\",\n",
    "    \"        'ret_rel': df['Retrocue Reliability'],\\n\",\n",
    "    \"        'validity_binary': df['Tested Item'].apply(lambda x: 1 if x == 'prioritized' else 0),\\n\",\n",
    "    \"        'reliability_binary': df['Retrocue Reliability'].apply(lambda x: 1 if x == 'high' else 0)\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add all new columns at once\\n\",\n",
    "    \"    for col_name, values in new_columns.items():\\n\",\n",
    "    \"        df_add[col_name] = values\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df_add\\n\",\n",
    "    \"\\n\",\n",
    "    \"def validity_assignment(df):\\n\",\n",
    "    \"    \\\"\\\"\\\"Create 'Tested Item' column based on validity.\\\"\\\"\\\"\\n\",\n",
    "    \"    df_validity = df.copy()\\n\",\n",
    "    \"    df_validity['Tested Item'] = np.where(df_validity['validity'] == 'valid', 'prioritized', 'deprioritized')\\n\",\n",
    "    \"    return df_validity\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply processing\\n\",\n",
    "    \"df = validity_assignment(df)\\n\",\n",
    "    \"df = categorize_columns(df, column_params)\\n\",\n",
    "    \"df = df_column_addition(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Added categorized columns and user interface labels. Total columns: {len(df.columns)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"standardize-markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Standardize Variables and Create Transformed Features\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"standardize-cell\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def df_demean(df, list_of_variables):\\n\",\n",
    "    \"    \\\"\\\"\\\"Demean variables and create z-scored versions.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Process all variables at once to avoid fragmentation\\n\",\n",
    "    \"    df_std = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Dictionary to store all new columns\\n\",\n",
    "    \"    new_columns = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for l in list_of_variables:\\n\",\n",
    "    \"        # Demean the variable\\n\",\n",
    "    \"        new_columns[l] = df_std[l] - np.mean(df_std[l])\\n\",\n",
    "    \"        # Create z-scored version\\n\",\n",
    "    \"        z_scored = f\\\"{l}_z\\\"\\n\",\n",
    "    \"        new_columns[z_scored] = scaler.fit_transform(pd.DataFrame(new_columns[squared_col])).flatten()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add all new columns at once\\n\",\n",
    "    \"    for col_name, values in new_columns.items():\\n\",\n",
    "    \"        df_sq[col_name] = values\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    return df_sq\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply standardization to similarity metrics\\n\",\n",
    "    \"sim_variables = [\\n\",\n",
    "    \"    'it_sim_dis_diff', 'v2_sim_dis_diff', \\n\",\n",
    "    \"    'it_sim_dis_attend', 'v2_sim_dis_attend',\\n\",\n",
    "    \"    'it_sim_dis_unattend', 'v2_sim_dis_unattend', \\n\",\n",
    "    \"    'it_sim_dis_test', 'v2_sim_dis_test',\\n\",\n",
    "    \"    'it_sim_dis_untest', 'v2_sim_dis_untest', \\n\",\n",
    "    \"    'it_sim_dis_diff_test', 'v2_sim_dis_diff_test'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply standardization\\n\",\n",
    "    \"df = df_demean(df, sim_variables)\\n\",\n",
    "    \"df = df_square_and_mean(df, sim_variables)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Update binary indicators to z-scored versions\\n\",\n",
    "    \"df['validity_binary'] = (df['validity'] == 'valid').astype(int)\\n\",\n",
    "    \"df['reliability_binary'] = (df['reliability'] > 0.7).astype(int)\\n\",\n",
    "    \"df['validity_binary_z'] = scaler.fit_transform(df[['validity_binary']])\\n\",\n",
    "    \"df['reliability_binary_z'] = scaler.fit_transform(df[['reliability_binary']])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create additional UI columns with z-scored values\\n\",\n",
    "    \"ui_columns = {\\n\",\n",
    "    \"    'V2 Distractor Similarity to Tested Item': df['v2_sim_dis_test_z'],\\n\",\n",
    "    \"    'IT Distractor Similarity to Tested Item': df['it_sim_dis_test_z'],\\n\",\n",
    "    \"    'Tested - Untested V2 Distractor Similarity': df['v2_sim_dis_diff_test_z'],\\n\",\n",
    "    \"    'Tested - Untested IT Distractor Similarity': df['it_sim_dis_diff_test_z'],\\n\",\n",
    "    \"    'V2 Distractor Similarity\\\\nto Prioritized Item': df['v2_sim_dis_attend_z'],\\n\",\n",
    "    \"    'IT Distractor Similarity\\\\nto Prioritized Item': df['it_sim_dis_attend_z'],\\n\",\n",
    "    \"    'V2 Distractor Similarity\\\\nto Deprioritized Item': df['v2_sim_dis_unattend_z'],\\n\",\n",
    "    \"    'IT Distractor Similarity\\\\nto Deprioritized Item': df['it_sim_dis_unattend_z'],\\n\",\n",
    "    \"    'Prioritized - Deprioritized IT Distractor Similarity': df['it_sim_dis_diff_z'],\\n\",\n",
    "    \"    'Prioritized - Deprioritized V2 Distractor Similarity': df['v2_sim_dis_diff_z']\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add UI columns at once\\n\",\n",
    "    \"for col_name, values in ui_columns.items():\\n\",\n",
    "    \"    df[col_name] = values\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Added standardized variables and transformations. Total columns: {len(df.columns)}\\\")_scored] = scaler.fit_transform(df_std[[l]]).flatten()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add all new columns at once\\n\",\n",
    "    \"    for col_name, values in new_columns.items():\\n\",\n",
    "    \"        df_std[col_name] = values\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    return df_std\\n\",\n",
    "    \"\\n\",\n",
    "    \"def df_square_and_mean(df, list_of_variables):\\n\",\n",
    "    \"    \\\"\\\"\\\"Square variables, demean them, and create z-scored versions.\\\"\\\"\\\"\\n\",\n",
    "    \"    # Process all variables at once to avoid fragmentation\\n\",\n",
    "    \"    df_sq = df.copy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Dictionary to store all new columns\\n\",\n",
    "    \"    new_columns = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for l in list_of_variables:\\n\",\n",
    "    \"        # Square the variable\\n\",\n",
    "    \"        squared_col = f\\\"{l}_sq\\\"\\n\",\n",
    "    \"        new_columns[squared_col] = df_sq[l]**2\\n\",\n",
    "    \"        # Demean the squared variable\\n\",\n",
    "    \"        new_columns[squared_col] = new_columns[squared_col] - np.mean(new_columns[squared_col])\\n\",\n",
    "    \"        # Create z-scored version\\n\",\n",
    "    \"        z_scored = f\\\"{squared_col}_z\\\"\\n\",\n",
    "    \"        new_columns[z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
