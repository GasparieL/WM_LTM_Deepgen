{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c35ba9e-32cb-4817-94e8-3fab629b2371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:cd:1: no such file or directory: new_stim\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ffde1d-9799-42c2-bbde-a84122bc93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b7d35de-3a2d-4324-b136-b2834c2e5ffd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for duplicate images in: stimuli\n",
      "This may take a while for large folders...\n",
      "\n",
      "\n",
      "============================================================\n",
      "SCAN RESULTS\n",
      "============================================================\n",
      "Total image files processed: 2489\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 1:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/219_1.jpg\n",
      "     Full path: stimuli/219_1.jpg\n",
      "  2. .../demo_stims/219_1.jpg\n",
      "     Full path: stimuli/demo_stims/219_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 2:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/219_2.jpg\n",
      "     Full path: stimuli/219_2.jpg\n",
      "  2. .../demo_stims/219_2.jpg\n",
      "     Full path: stimuli/demo_stims/219_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 3:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/219_3.jpg\n",
      "     Full path: stimuli/219_3.jpg\n",
      "  2. .../demo_stims/219_3.jpg\n",
      "     Full path: stimuli/demo_stims/219_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 4:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/281_2.jpg\n",
      "     Full path: stimuli/281_2.jpg\n",
      "  2. .../demo_stims/281_2.jpg\n",
      "     Full path: stimuli/demo_stims/281_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 5:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/281_3.jpg\n",
      "     Full path: stimuli/281_3.jpg\n",
      "  2. .../demo_stims/281_3.jpg\n",
      "     Full path: stimuli/demo_stims/281_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 6:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/142_1.jpg\n",
      "     Full path: stimuli/142_1.jpg\n",
      "  2. .../demo_stims/142_1.jpg\n",
      "     Full path: stimuli/demo_stims/142_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 7:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/142_3.jpg\n",
      "     Full path: stimuli/142_3.jpg\n",
      "  2. .../demo_stims/142_3.jpg\n",
      "     Full path: stimuli/demo_stims/142_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 8:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/281_1.jpg\n",
      "     Full path: stimuli/281_1.jpg\n",
      "  2. .../demo_stims/281_1.jpg\n",
      "     Full path: stimuli/demo_stims/281_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 9:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/142_2.jpg\n",
      "     Full path: stimuli/142_2.jpg\n",
      "  2. .../demo_stims/142_2.jpg\n",
      "     Full path: stimuli/demo_stims/142_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 10:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/201_3.jpg\n",
      "     Full path: stimuli/201_3.jpg\n",
      "  2. .../demo_stims/201_3.jpg\n",
      "     Full path: stimuli/demo_stims/201_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 11:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/187_1.jpg\n",
      "     Full path: stimuli/187_1.jpg\n",
      "  2. .../demo_stims/187_1.jpg\n",
      "     Full path: stimuli/demo_stims/187_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 12:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/201_2.jpg\n",
      "     Full path: stimuli/201_2.jpg\n",
      "  2. .../demo_stims/201_2.jpg\n",
      "     Full path: stimuli/demo_stims/201_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 13:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/187_2.jpg\n",
      "     Full path: stimuli/187_2.jpg\n",
      "  2. .../demo_stims/187_2.jpg\n",
      "     Full path: stimuli/demo_stims/187_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 14:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/187_3.jpg\n",
      "     Full path: stimuli/187_3.jpg\n",
      "  2. .../demo_stims/187_3.jpg\n",
      "     Full path: stimuli/demo_stims/187_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 15:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/201_1.jpg\n",
      "     Full path: stimuli/201_1.jpg\n",
      "  2. .../demo_stims/201_1.jpg\n",
      "     Full path: stimuli/demo_stims/201_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 16:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/167_2.jpg\n",
      "     Full path: stimuli/167_2.jpg\n",
      "  2. .../demo_stims/167_2.jpg\n",
      "     Full path: stimuli/demo_stims/167_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 17:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/240_1.jpg\n",
      "     Full path: stimuli/240_1.jpg\n",
      "  2. .../demo_stims/240_1.jpg\n",
      "     Full path: stimuli/demo_stims/240_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 18:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/167_3.jpg\n",
      "     Full path: stimuli/167_3.jpg\n",
      "  2. .../demo_stims/167_3.jpg\n",
      "     Full path: stimuli/demo_stims/167_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 19:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/167_1.jpg\n",
      "     Full path: stimuli/167_1.jpg\n",
      "  2. .../demo_stims/167_1.jpg\n",
      "     Full path: stimuli/demo_stims/167_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 20:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/240_2.jpg\n",
      "     Full path: stimuli/240_2.jpg\n",
      "  2. .../demo_stims/240_2.jpg\n",
      "     Full path: stimuli/demo_stims/240_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 21:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/240_3.jpg\n",
      "     Full path: stimuli/240_3.jpg\n",
      "  2. .../demo_stims/240_3.jpg\n",
      "     Full path: stimuli/demo_stims/240_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 22:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/83_2.jpg\n",
      "     Full path: stimuli/83_2.jpg\n",
      "  2. .../demo_stims/83_2.jpg\n",
      "     Full path: stimuli/demo_stims/83_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 23:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/83_3.jpg\n",
      "     Full path: stimuli/83_3.jpg\n",
      "  2. .../demo_stims/83_3.jpg\n",
      "     Full path: stimuli/demo_stims/83_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 24:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/83_1.jpg\n",
      "     Full path: stimuli/83_1.jpg\n",
      "  2. .../demo_stims/83_1.jpg\n",
      "     Full path: stimuli/demo_stims/83_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 25:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/149_1.jpg\n",
      "     Full path: stimuli/149_1.jpg\n",
      "  2. .../demo_stims/149_1.jpg\n",
      "     Full path: stimuli/demo_stims/149_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 26:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/78_2.jpg\n",
      "     Full path: stimuli/78_2.jpg\n",
      "  2. .../demo_stims/78_2.jpg\n",
      "     Full path: stimuli/demo_stims/78_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 27:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/216_2.jpg\n",
      "     Full path: stimuli/216_2.jpg\n",
      "  2. .../demo_stims/216_2.jpg\n",
      "     Full path: stimuli/demo_stims/216_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 28:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/189_1.jpg\n",
      "     Full path: stimuli/189_1.jpg\n",
      "  2. .../demo_stims/189_1.jpg\n",
      "     Full path: stimuli/demo_stims/189_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 29:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/216_3.jpg\n",
      "     Full path: stimuli/216_3.jpg\n",
      "  2. .../demo_stims/216_3.jpg\n",
      "     Full path: stimuli/demo_stims/216_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 30:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/78_3.jpg\n",
      "     Full path: stimuli/78_3.jpg\n",
      "  2. .../demo_stims/78_3.jpg\n",
      "     Full path: stimuli/demo_stims/78_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 31:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/149_2.jpg\n",
      "     Full path: stimuli/149_2.jpg\n",
      "  2. .../demo_stims/149_2.jpg\n",
      "     Full path: stimuli/demo_stims/149_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 32:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/78_1.jpg\n",
      "     Full path: stimuli/78_1.jpg\n",
      "  2. .../demo_stims/78_1.jpg\n",
      "     Full path: stimuli/demo_stims/78_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 33:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/216_1.jpg\n",
      "     Full path: stimuli/216_1.jpg\n",
      "  2. .../demo_stims/216_1.jpg\n",
      "     Full path: stimuli/demo_stims/216_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 34:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/189_2.jpg\n",
      "     Full path: stimuli/189_2.jpg\n",
      "  2. .../demo_stims/189_2.jpg\n",
      "     Full path: stimuli/demo_stims/189_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 35:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/189_3.jpg\n",
      "     Full path: stimuli/189_3.jpg\n",
      "  2. .../demo_stims/189_3.jpg\n",
      "     Full path: stimuli/demo_stims/189_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 36:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/149_3.jpg\n",
      "     Full path: stimuli/149_3.jpg\n",
      "  2. .../demo_stims/149_3.jpg\n",
      "     Full path: stimuli/demo_stims/149_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 37:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/270_1.jpg\n",
      "     Full path: stimuli/270_1.jpg\n",
      "  2. .../demo_stims/270_1.jpg\n",
      "     Full path: stimuli/demo_stims/270_1.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 38:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/270_3.jpg\n",
      "     Full path: stimuli/270_3.jpg\n",
      "  2. .../demo_stims/270_3.jpg\n",
      "     Full path: stimuli/demo_stims/270_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 39:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/270_2.jpg\n",
      "     Full path: stimuli/270_2.jpg\n",
      "  2. .../demo_stims/270_2.jpg\n",
      "     Full path: stimuli/demo_stims/270_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 40:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/114_3.jpg\n",
      "     Full path: stimuli/114_3.jpg\n",
      "  2. .../demo_stims/114_3.jpg\n",
      "     Full path: stimuli/demo_stims/114_3.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 41:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/114_2.jpg\n",
      "     Full path: stimuli/114_2.jpg\n",
      "  2. .../demo_stims/114_2.jpg\n",
      "     Full path: stimuli/demo_stims/114_2.jpg\n",
      "\n",
      "----------------------------------------\n",
      "Duplicate Group 42:\n",
      "Found 2 identical images:\n",
      "  1. .../stimuli/114_1.jpg\n",
      "     Full path: stimuli/114_1.jpg\n",
      "  2. .../demo_stims/114_1.jpg\n",
      "     Full path: stimuli/demo_stims/114_1.jpg\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Found 42 groups of duplicate images\n",
      "Total duplicate files: 42\n",
      "Space could be saved by removing 42 files\n",
      "============================================================\n",
      "\n",
      "Additional Statistics:\n",
      "Unique images found: 2447\n",
      "\n",
      "File types found:\n",
      "  .jpg: 2486 files\n",
      "  .png: 3 files\n",
      "\n",
      "Duplicate Files Summary (showing in tabular format):\n",
      " Group  File_Number  Filename          Directory\n",
      "     1            1 219_1.jpg            stimuli\n",
      "     1            2 219_1.jpg stimuli/demo_stims\n",
      "     2            1 219_2.jpg            stimuli\n",
      "     2            2 219_2.jpg stimuli/demo_stims\n",
      "     3            1 219_3.jpg            stimuli\n",
      "     3            2 219_3.jpg stimuli/demo_stims\n",
      "     4            1 281_2.jpg            stimuli\n",
      "     4            2 281_2.jpg stimuli/demo_stims\n",
      "     5            1 281_3.jpg            stimuli\n",
      "     5            2 281_3.jpg stimuli/demo_stims\n",
      "     6            1 142_1.jpg            stimuli\n",
      "     6            2 142_1.jpg stimuli/demo_stims\n",
      "     7            1 142_3.jpg            stimuli\n",
      "     7            2 142_3.jpg stimuli/demo_stims\n",
      "     8            1 281_1.jpg            stimuli\n",
      "     8            2 281_1.jpg stimuli/demo_stims\n",
      "     9            1 142_2.jpg            stimuli\n",
      "     9            2 142_2.jpg stimuli/demo_stims\n",
      "    10            1 201_3.jpg            stimuli\n",
      "    10            2 201_3.jpg stimuli/demo_stims\n",
      "    11            1 187_1.jpg            stimuli\n",
      "    11            2 187_1.jpg stimuli/demo_stims\n",
      "    12            1 201_2.jpg            stimuli\n",
      "    12            2 201_2.jpg stimuli/demo_stims\n",
      "    13            1 187_2.jpg            stimuli\n",
      "    13            2 187_2.jpg stimuli/demo_stims\n",
      "    14            1 187_3.jpg            stimuli\n",
      "    14            2 187_3.jpg stimuli/demo_stims\n",
      "    15            1 201_1.jpg            stimuli\n",
      "    15            2 201_1.jpg stimuli/demo_stims\n",
      "    16            1 167_2.jpg            stimuli\n",
      "    16            2 167_2.jpg stimuli/demo_stims\n",
      "    17            1 240_1.jpg            stimuli\n",
      "    17            2 240_1.jpg stimuli/demo_stims\n",
      "    18            1 167_3.jpg            stimuli\n",
      "    18            2 167_3.jpg stimuli/demo_stims\n",
      "    19            1 167_1.jpg            stimuli\n",
      "    19            2 167_1.jpg stimuli/demo_stims\n",
      "    20            1 240_2.jpg            stimuli\n",
      "    20            2 240_2.jpg stimuli/demo_stims\n",
      "    21            1 240_3.jpg            stimuli\n",
      "    21            2 240_3.jpg stimuli/demo_stims\n",
      "    22            1  83_2.jpg            stimuli\n",
      "    22            2  83_2.jpg stimuli/demo_stims\n",
      "    23            1  83_3.jpg            stimuli\n",
      "    23            2  83_3.jpg stimuli/demo_stims\n",
      "    24            1  83_1.jpg            stimuli\n",
      "    24            2  83_1.jpg stimuli/demo_stims\n",
      "    25            1 149_1.jpg            stimuli\n",
      "    25            2 149_1.jpg stimuli/demo_stims\n",
      "    26            1  78_2.jpg            stimuli\n",
      "    26            2  78_2.jpg stimuli/demo_stims\n",
      "    27            1 216_2.jpg            stimuli\n",
      "    27            2 216_2.jpg stimuli/demo_stims\n",
      "    28            1 189_1.jpg            stimuli\n",
      "    28            2 189_1.jpg stimuli/demo_stims\n",
      "    29            1 216_3.jpg            stimuli\n",
      "    29            2 216_3.jpg stimuli/demo_stims\n",
      "    30            1  78_3.jpg            stimuli\n",
      "    30            2  78_3.jpg stimuli/demo_stims\n",
      "    31            1 149_2.jpg            stimuli\n",
      "    31            2 149_2.jpg stimuli/demo_stims\n",
      "    32            1  78_1.jpg            stimuli\n",
      "    32            2  78_1.jpg stimuli/demo_stims\n",
      "    33            1 216_1.jpg            stimuli\n",
      "    33            2 216_1.jpg stimuli/demo_stims\n",
      "    34            1 189_2.jpg            stimuli\n",
      "    34            2 189_2.jpg stimuli/demo_stims\n",
      "    35            1 189_3.jpg            stimuli\n",
      "    35            2 189_3.jpg stimuli/demo_stims\n",
      "    36            1 149_3.jpg            stimuli\n",
      "    36            2 149_3.jpg stimuli/demo_stims\n",
      "    37            1 270_1.jpg            stimuli\n",
      "    37            2 270_1.jpg stimuli/demo_stims\n",
      "    38            1 270_3.jpg            stimuli\n",
      "    38            2 270_3.jpg stimuli/demo_stims\n",
      "    39            1 270_2.jpg            stimuli\n",
      "    39            2 270_2.jpg stimuli/demo_stims\n",
      "    40            1 114_3.jpg            stimuli\n",
      "    40            2 114_3.jpg stimuli/demo_stims\n",
      "    41            1 114_2.jpg            stimuli\n",
      "    41            2 114_2.jpg stimuli/demo_stims\n",
      "    42            1 114_1.jpg            stimuli\n",
      "    42            2 114_1.jpg stimuli/demo_stims\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_image_hash(filepath):\n",
    "    \"\"\"\n",
    "    Calculate hash of an image file.\n",
    "    Returns None if file cannot be opened as image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            # Convert to bytes for hashing\n",
    "            img_bytes = img.tobytes()\n",
    "            return hashlib.md5(img_bytes).hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_duplicate_images(folder_path):\n",
    "    \"\"\"\n",
    "    Find duplicate images in folder and all subfolders.\n",
    "    Returns a dictionary where keys are hashes and values are lists of file paths.\n",
    "    \"\"\"\n",
    "    # Dictionary to store hash -> list of file paths\n",
    "    image_hashes = defaultdict(list)\n",
    "    \n",
    "    # Supported image extensions\n",
    "    supported_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if file has image extension\n",
    "            _, ext = os.path.splitext(file.lower())\n",
    "            if ext in supported_extensions:\n",
    "                filepath = os.path.join(root, file)\n",
    "                # print(f\"Processing: {filepath}\")\n",
    "                \n",
    "                # Calculate hash\n",
    "                img_hash = get_image_hash(filepath)\n",
    "                if img_hash:\n",
    "                    image_hashes[img_hash].append(filepath)\n",
    "    \n",
    "    return image_hashes\n",
    "\n",
    "def analyze_duplicates(image_hashes):\n",
    "    \"\"\"\n",
    "    Analyze and print information about duplicate images.\n",
    "    \"\"\"\n",
    "    total_files = sum(len(paths) for paths in image_hashes.values())\n",
    "    duplicate_groups = 0\n",
    "    duplicate_files = 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SCAN RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total image files processed: {total_files}\")\n",
    "    \n",
    "    duplicates_found = False\n",
    "    \n",
    "    for hash_value, file_paths in image_hashes.items():\n",
    "        if len(file_paths) > 1:\n",
    "            duplicates_found = True\n",
    "            duplicate_groups += 1\n",
    "            duplicate_files += len(file_paths) - 1  # Don't count the original\n",
    "            \n",
    "            print(f\"\\n{'-'*40}\")\n",
    "            print(f\"Duplicate Group {duplicate_groups}:\")\n",
    "            print(f\"Found {len(file_paths)} identical images:\")\n",
    "            \n",
    "            for i, path in enumerate(file_paths, 1):\n",
    "                # Extract just the filename and parent folder for cleaner display\n",
    "                parts = path.split(os.sep)\n",
    "                if len(parts) >= 2:\n",
    "                    display_path = os.path.join(\"...\", parts[-2], parts[-1])\n",
    "                else:\n",
    "                    display_path = parts[-1]\n",
    "                print(f\"  {i}. {display_path}\")\n",
    "                print(f\"     Full path: {path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    if duplicates_found:\n",
    "        print(f\"Found {duplicate_groups} groups of duplicate images\")\n",
    "        print(f\"Total duplicate files: {duplicate_files}\")\n",
    "        print(f\"Space could be saved by removing {duplicate_files} files\")\n",
    "    else:\n",
    "        print(\"âœ“ No duplicate images found!\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Main execution cell\n",
    "# Replace this path with your actual folder path\n",
    "FOLDER_TO_SCAN = \"stimuli\"  # Change this to your folder path\n",
    "\n",
    "# Verify the folder exists\n",
    "if not os.path.exists(FOLDER_TO_SCAN):\n",
    "    print(f\"Error: Folder '{FOLDER_TO_SCAN}' does not exist!\")\n",
    "    print(\"Please update the FOLDER_TO_SCAN variable with the correct path.\")\n",
    "else:\n",
    "    print(f\"Scanning for duplicate images in: {FOLDER_TO_SCAN}\")\n",
    "    print(\"This may take a while for large folders...\\n\")\n",
    "    \n",
    "    # Find duplicates\n",
    "    image_hashes = find_duplicate_images(FOLDER_TO_SCAN)\n",
    "    \n",
    "    # Analyze and display results\n",
    "    analyze_duplicates(image_hashes)\n",
    "\n",
    "# Optional: Display additional statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Unique images found: {len(image_hashes)}\")\n",
    "\n",
    "# Count files by extension\n",
    "extensions_count = defaultdict(int)\n",
    "for hash_value, file_paths in image_hashes.items():\n",
    "    for filepath in file_paths:\n",
    "        _, ext = os.path.splitext(filepath.lower())\n",
    "        extensions_count[ext] += 1\n",
    "\n",
    "print(\"\\nFile types found:\")\n",
    "for ext, count in sorted(extensions_count.items()):\n",
    "    print(f\"  {ext}: {count} files\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a list of all duplicate files for DataFrame\n",
    "duplicate_data = []\n",
    "group_number = 1\n",
    "\n",
    "for hash_value, file_paths in image_hashes.items():\n",
    "    if len(file_paths) > 1:\n",
    "        for i, path in enumerate(file_paths):\n",
    "            duplicate_data.append({\n",
    "                'Group': group_number,\n",
    "                'File_Number': i + 1,\n",
    "                'Filename': os.path.basename(path),\n",
    "                'Directory': os.path.dirname(path),\n",
    "                # 'Full_Path': path,\n",
    "                # 'Hash': hash_value[:10] + '...'  # Show only first 10 chars of hash\n",
    "            })\n",
    "        group_number += 1\n",
    "\n",
    "if duplicate_data:\n",
    "    df_duplicates = pd.DataFrame(duplicate_data)\n",
    "    print(\"\\nDuplicate Files Summary (showing in tabular format):\")\n",
    "    print(df_duplicates.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV if desired\n",
    "    # df_duplicates.to_csv('duplicate_images_report.csv', index=False)\n",
    "    # print(\"\\nReport saved to 'duplicate_images_report.csv'\")\n",
    "else:\n",
    "    print(\"\\nNo duplicates to display in table format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e1b47c4-06b1-4439-b07c-1eb1d07ebce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_hash(filepath):\n",
    "    \"\"\"Calculate hash of an image file.\"\"\"\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            img_bytes = img.tobytes()\n",
    "            return hashlib.md5(img_bytes).hexdigest()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_duplicate_images(folder_path):\n",
    "    \"\"\"Find duplicate images in folder and all subfolders.\"\"\"\n",
    "    image_hashes = defaultdict(list)\n",
    "    supported_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            _, ext = os.path.splitext(file.lower())\n",
    "            if ext in supported_extensions:\n",
    "                filepath = os.path.join(root, file)\n",
    "                # print(f\"Processing: {filepath}\")\n",
    "                \n",
    "                img_hash = get_image_hash(filepath)\n",
    "                if img_hash:\n",
    "                    image_hashes[img_hash].append(filepath)\n",
    "    \n",
    "    return image_hashes\n",
    "\n",
    "# 2. SAVE DUPLICATE LIST\n",
    "def save_duplicate_list(image_hashes, output_path=\"duplicate_images_list.json\"):\n",
    "    \"\"\"Save list of duplicate images to JSON file.\"\"\"\n",
    "    duplicates_data = {\n",
    "        \"scan_date\": datetime.now().isoformat(),\n",
    "        \"duplicate_groups\": []\n",
    "    }\n",
    "    \n",
    "    group_id = 1\n",
    "    all_duplicates = []\n",
    "    \n",
    "    for hash_value, file_paths in image_hashes.items():\n",
    "        if len(file_paths) > 1:\n",
    "            group_data = {\n",
    "                \"group_id\": group_id,\n",
    "                \"hash\": hash_value,\n",
    "                \"files\": file_paths\n",
    "            }\n",
    "            duplicates_data[\"duplicate_groups\"].append(group_data)\n",
    "            \n",
    "            # Add all files in this group to our flat list\n",
    "            all_duplicates.extend(file_paths)\n",
    "            group_id += 1\n",
    "    \n",
    "    # Save JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(duplicates_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDuplicate list saved to: {output_path}\")\n",
    "    print(f\"Total duplicate files: {len(all_duplicates)}\")\n",
    "    \n",
    "    return all_duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85223250-2fa1-4ae9-b9d7-ffc18fe3c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_files_in_csv(file_list, csv_paths, folder_path):\n",
    "    \"\"\"Check if files appear in the specified CSV files.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CHECKING DUPLICATES IN CSV FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        # CSV files are in the current working directory, not in folder_path\n",
    "        full_csv_path = os.path.join(os.getcwd(), csv_path)\n",
    "        \n",
    "        if not os.path.exists(full_csv_path):\n",
    "            print(f\"\\nWarning: {csv_path} not found in {folder_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(full_csv_path)\n",
    "            print(f\"\\nChecking {csv_path}...\")\n",
    "            print(f\"CSV shape: {df.shape}\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Look for columns that might contain file paths\n",
    "            potential_path_columns = []\n",
    "            for col in df.columns:\n",
    "                # Check if column contains file paths\n",
    "                if col.lower() in ['path', 'file', 'filename', 'filepath', 'image', 'source']:\n",
    "                    potential_path_columns.append(col)\n",
    "                elif df[col].dtype == 'object':\n",
    "                    # Check if values look like file paths\n",
    "                    sample_values = df[col].dropna().head(5).astype(str)\n",
    "                    if any('/' in str(val) or '\\\\' in str(val) or '.' in str(val) for val in sample_values):\n",
    "                        potential_path_columns.append(col)\n",
    "            \n",
    "            if not potential_path_columns:\n",
    "                print(f\"No obvious path columns found in {csv_path}\")\n",
    "                print(\"All columns:\", list(df.columns))\n",
    "                # Let user choose columns to check\n",
    "                continue\n",
    "            \n",
    "            # Check each potential path column\n",
    "            found_files = set()\n",
    "            for col in potential_path_columns:\n",
    "                print(f\"\\nChecking column '{col}'...\")\n",
    "                \n",
    "                for file_path in file_list:\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    \n",
    "                    # Check if filename appears in this column\n",
    "                    matches = df[df[col].astype(str).str.contains(filename, na=False, case=False)]\n",
    "                    \n",
    "                    if not matches.empty:\n",
    "                        found_files.add(file_path)\n",
    "                        print(f\"  Found: {filename}\")\n",
    "                        print(f\"    Rows: {len(matches)}\")\n",
    "                        for idx, row in matches.head(3).iterrows():\n",
    "                            print(f\"    Example: {row[col]}\")\n",
    "            \n",
    "            results[csv_path] = {\n",
    "                'total_files_checked': len(file_list),\n",
    "                'files_found': len(found_files),\n",
    "                'found_files': list(found_files),\n",
    "                'columns_checked': potential_path_columns\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nSummary for {csv_path}:\")\n",
    "            print(f\"  Files found: {len(found_files)} out of {len(file_list)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_path}: {e}\")\n",
    "            results[csv_path] = {'error': str(e)}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9213a87d-24f1-4ae4-a013-32dcd79ca1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for duplicate images in: new_stimuli\n",
      "This may take a while for large folders...\n",
      "\n",
      "\n",
      "Duplicate list saved to: duplicate_images_list.json\n",
      "Total duplicate files: 28\n",
      "\n",
      "============================================================\n",
      "CHECKING DUPLICATES IN CSV FILES\n",
      "============================================================\n",
      "\n",
      "Checking selected_and_foils.csv...\n",
      "CSV shape: (240, 1)\n",
      "Columns: ['ImagePath']\n",
      "\n",
      "Checking column 'ImagePath'...\n",
      "  Found: 97_3.jpg\n",
      "    Rows: 1\n",
      "    Example: ./stimuli/297_3.jpg\n",
      "\n",
      "Summary for selected_and_foils.csv:\n",
      "  Files found: 1 out of 28\n",
      "\n",
      "CSV check results saved to: csv_check_results.json\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "\n",
      "selected_and_foils.csv:\n",
      "  Duplicates found: 1/28\n",
      "  Found files:\n",
      "    - 97_3.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. MAIN EXECUTION\n",
    "# Configure paths\n",
    "FOLDER_TO_SCAN = \"new_stimuli\"  # Change this to your folder path\n",
    "# CSV_FILES = [\"initial_practice.csv\", \"prac.csv\", \"demo.csv\"]\n",
    "CSV_FILES = [\"selected_and_foils.csv\"]\n",
    "# Verify the folder exists\n",
    "if not os.path.exists(FOLDER_TO_SCAN):\n",
    "    print(f\"Error: Folder '{FOLDER_TO_SCAN}' does not exist!\")\n",
    "    print(\"Please update the FOLDER_TO_SCAN variable with the correct path.\")\n",
    "else:\n",
    "    print(f\"Scanning for duplicate images in: {FOLDER_TO_SCAN}\")\n",
    "    print(\"This may take a while for large folders...\\n\")\n",
    "    \n",
    "    # Step 1: Find duplicates\n",
    "    image_hashes = find_duplicate_images(FOLDER_TO_SCAN)\n",
    "    \n",
    "    # Step 2: Save duplicate list\n",
    "    duplicate_files = save_duplicate_list(image_hashes)\n",
    "    \n",
    "    # Step 3: Check if duplicates appear in CSV files\n",
    "    if duplicate_files:\n",
    "        csv_results = check_files_in_csv(duplicate_files, CSV_FILES, FOLDER_TO_SCAN)\n",
    "        \n",
    "        # Save CSV check results\n",
    "        csv_results_path = \"csv_check_results.json\"\n",
    "        with open(csv_results_path, 'w') as f:\n",
    "            json.dump(csv_results, f, indent=2)\n",
    "        print(f\"\\nCSV check results saved to: {csv_results_path}\")\n",
    "        \n",
    "        # Create summary report\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FINAL SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for csv_file, result in csv_results.items():\n",
    "            if 'error' not in result:\n",
    "                print(f\"\\n{csv_file}:\")\n",
    "                print(f\"  Duplicates found: {result['files_found']}/{result['total_files_checked']}\")\n",
    "                if result['files_found'] > 0:\n",
    "                    print(f\"  Found files:\")\n",
    "                    for file_path in result['found_files']:  # Show first 5\n",
    "                        print(f\"    - {os.path.basename(file_path)}\")\n",
    "                    # if len(result['found_files']) > 5:\n",
    "                    #     print(f\"    ... and {len(result['found_files']) - 5} more\")\n",
    "    else:\n",
    "        print(\"\\nNo duplicate images found, so nothing to check in CSV files.\")\n",
    "\n",
    "# # 5. OPTIONAL: More detailed analysis\n",
    "# def analyze_csv_structure(csv_paths, folder_path):\n",
    "#     \"\"\"Analyze the structure of CSV files to help identify relevant columns.\"\"\"\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"CSV STRUCTURE ANALYSIS\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     for csv_path in csv_paths:\n",
    "#         # CSV files are in the current working directory, not in folder_path\n",
    "#         full_csv_path = os.path.join(os.getcwd(), csv_path)\n",
    "        \n",
    "#         if os.path.exists(full_csv_path):\n",
    "#             try:\n",
    "#                 df = pd.read_csv(full_csv_path, nrows=10)  # Read only first 10 rows for analysis\n",
    "#                 print(f\"\\n{csv_path}:\")\n",
    "#                 print(f\"  Shape: {df.shape}\")\n",
    "#                 print(f\"  Columns: {list(df.columns)}\")\n",
    "#                 print(f\"  Sample data:\")\n",
    "#                 for col in df.columns:\n",
    "#                     print(f\"    {col}: {df[col].iloc[0] if not df.empty else 'N/A'}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"\\nError analyzing {csv_path}: {e}\")\n",
    "\n",
    "# Run CSV structure analysis\n",
    "# analyze_csv_structure(CSV_FILES, FOLDER_TO_SCAN)\n",
    "\n",
    "# # 6. INTERACTIVE COLUMN SELECTION (if needed)\n",
    "# def check_specific_columns(file_list, csv_paths, folder_path, columns_to_check):\n",
    "#     \"\"\"Check specific columns in CSV files.\"\"\"\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"CHECKING SPECIFIC COLUMNS\")\n",
    "#     print(f\"{'='*60}\")\n",
    "    \n",
    "#     for csv_path in csv_paths:\n",
    "#         # CSV files are in the current working directory, not in folder_path\n",
    "#         full_csv_path = os.path.join(os.getcwd(), csv_path)\n",
    "        \n",
    "#         if os.path.exists(full_csv_path):\n",
    "#             try:\n",
    "#                 df = pd.read_csv(full_csv_path)\n",
    "#                 print(f\"\\nChecking {csv_path}, columns: {columns_to_check}\")\n",
    "                \n",
    "#                 for col in columns_to_check:\n",
    "#                     if col in df.columns:\n",
    "#                         print(f\"\\nColumn '{col}':\")\n",
    "#                         for file_path in file_list[:5]:  # Check first 5 files\n",
    "#                             filename = os.path.basename(file_path)\n",
    "#                             matches = df[df[col].astype(str).str.contains(filename, na=False, case=False)]\n",
    "#                             if not matches.empty:\n",
    "#                                 print(f\"  Found {filename}: {len(matches)} matches\")\n",
    "#                     else:\n",
    "#                         print(f\"\\nColumn '{col}' not found in {csv_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error: {e}\")\n",
    "\n",
    "# # Example: Uncomment and modify to check specific columns\n",
    "# # check_specific_columns(duplicate_files, CSV_FILES, FOLDER_TO_SCAN, ['filename', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1361236-102e-42ba-8322-e579e7edce9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
