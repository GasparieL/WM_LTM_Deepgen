{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f142231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "# from resmem import ResMem, transformer\n",
    "# from resmem.analysis import SaveFeatures\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "459a44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResMem(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca654afb-66a5-4c03-a284-15d77b64de29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining Data Timeframes.txt\n",
      "Heatmaps.ipynb\n",
      "Memorability_resmem.ipynb\n",
      "OSF Registries | View Registration Drafts | Fill out registration form.html\n",
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "Untitled2.ipynb\n",
      "WM_Deepgen copy.py\n",
      "WM_Deepgen-legacy-browsers.js\n",
      "WM_Deepgen.js\n",
      "WM_Deepgen.psyexp\n",
      "WM_Deepgen.py\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m\n",
      "analysis_july_batch1.ipynb\n",
      "analysis_july_batch1_stats.ipynb\n",
      "analysis_july_batch3-Copy1.ipynb\n",
      "analysis_july_batch3.ipynb\n",
      "analysis_july_batch3_stats.ipynb\n",
      "analysis_july_batch4_shuffled_data_randomShuffle.ipynb\n",
      "analysis_july_batch5.ipynb\n",
      "analysis_july_batch5_added_plots.ipynb\n",
      "analysis_july_batch5_bars_only.ipynb\n",
      "analysis_july_batch5_changed_similarity.ipynb\n",
      "analysis_july_batch5_changed_similarity_new_style.ipynb\n",
      "analysis_july_batch5_stats.ipynb\n",
      "analysis_july_batch6_added_plots.ipynb\n",
      "analysis_july_batch6_changed_similarity_new_style.ipynb\n",
      "analysis_ltm_batch5.ipynb\n",
      "analysis_ltm_batch5_test-wrt-distractor-Copy1.ipynb\n",
      "analysis_ltm_batch5_test-wrt-distractor.ipynb\n",
      "analysis_ltm_batch5_test.ipynb\n",
      "analysis_ltm_batch6.ipynb\n",
      "analysis_ltm_batch6_test-wrt-distractor.ipynb\n",
      "analysis_ltm_batch6_test.ipynb\n",
      "batch_3_stats.ipynb\n",
      "batch_5_stats.ipynb\n",
      "batch_5_stats_archive.ipynb\n",
      "batch_5_stats_correct_wm_ltm.csv-Copy1.ipynb\n",
      "batch_5_stats_correct_wm_ltm.csv.ipynb\n",
      "batch_5_stats_ltm_distractor.csv.ipynb\n",
      "batch_6_stats.ipynb\n",
      "batch_6_stats_correct_wm_ltm.csv.ipynb\n",
      "batch_6_stats_ltm_distractor.csv.ipynb\n",
      "conditions_file.csv\n",
      "conditions_file_new.csv\n",
      "conditions_shuffling.ipynb\n",
      "correlation_matrices_batch5-Copy1.ipynb\n",
      "correlation_matrices_batch5-c.ipynb\n",
      "correlation_matrices_batch5.ipynb\n",
      "correlation_matrices_batch6.ipynb\n",
      "correlation_matrices_both-Copy1.ipynb\n",
      "correlation_matrices_both.ipynb\n",
      "correlation_matrix_regressors.csv\n",
      "create_new_triplets.ipynb\n",
      "\u001b[34mdata\u001b[m\u001b[m\n",
      "data_qa.ipynb\n",
      "data_qa_archive.ipynb\n",
      "demo.csv\n",
      "df_ltm.csv\n",
      "eight_categories_creation.ipynb\n",
      "\u001b[34mfiles_ill need\u001b[m\u001b[m\n",
      "\u001b[34mfoil_stims\u001b[m\u001b[m\n",
      "image_subset_creation_random.ipynb\n",
      "index.html\n",
      "initial_practice.csv\n",
      "\u001b[34mjspsych-plugins\u001b[m\u001b[m\n",
      "last_part_of_experiment.ipynb\n",
      "ltm_data_creation.ipynb\n",
      "memorability_amnet.py\n",
      "memory_stims_with_foils.csv\n",
      "\u001b[34mnew_stimuli\u001b[m\u001b[m\n",
      "new_triplets.csv\n",
      "\u001b[34mnode_modules\u001b[m\u001b[m\n",
      "old_dataframe.csv\n",
      "only memory experiment data.ipynb\n",
      "only memory experiment data_after_April.ipynb\n",
      "package-lock.json\n",
      "package.json\n",
      "pilot1_nonresp_inc.csv\n",
      "pilot2_nonresp_inc.csv\n",
      "pilot3_nonresp_inc.csv\n",
      "pilot3x_nonresp_exc.csv\n",
      "pilot3x_nonresp_inc.csv\n",
      "pilot3x_nonresp_inc_stats.csv\n",
      "pilot4_shuffled_nonresp_inc.csv\n",
      "pilot5_nonresp_inc_test_diff.csv\n",
      "pilot5_nonresp_inc_test_diff_stats.csv\n",
      "pilot5_shuffled_nonresp_exc.csv\n",
      "pilot5_shuffled_nonresp_inc.csv\n",
      "pilot5_shuffled_nonresp_inc_ltm.csv\n",
      "pilot5_shuffled_nonresp_inc_stats.csv\n",
      "\u001b[34mpilot5_stats\u001b[m\u001b[m\n",
      "pilot5_unprocessed.csv\n",
      "pilot6_nonresp_inc_stats.csv\n",
      "pilot6_nonresp_inc_test_diff.csv\n",
      "pilot6_nonresp_inc_test_diff_stats.csv\n",
      "\u001b[34mpilot6_stats\u001b[m\u001b[m\n",
      "pilot6_unprocessed.csv\n",
      "pilot8_shuffled_nonresp_exc.csv\n",
      "pilot8_shuffled_nonresp_inc.csv\n",
      "\u001b[34mpowerai-image-memorability\u001b[m\u001b[m\n",
      "prac.csv\n",
      "preprocess_data.ipynb\n",
      "\u001b[34mquestionnaires\u001b[m\u001b[m\n",
      "questionnaires.html\n",
      "questionnaires.js\n",
      "random_image_component-legacy-browsers.js\n",
      "random_image_component.js\n",
      "random_image_component.psyexp\n",
      "random_image_component_lastrun.py\n",
      "readme.md\n",
      "resize_things_db.ipynb\n",
      "\u001b[34mresources\u001b[m\u001b[m\n",
      "\u001b[34mresources_pilot5\u001b[m\u001b[m\n",
      "\u001b[34mresources_pilot6\u001b[m\u001b[m\n",
      "\u001b[34mresources_pilot7\u001b[m\u001b[m\n",
      "\u001b[34mselected_and_foils\u001b[m\u001b[m\n",
      "selected_and_foils.csv\n",
      "\u001b[34mselected_and_foils_new\u001b[m\u001b[m\n",
      "selected_and_foils_new.csv\n",
      "selected_groups.csv\n",
      "\u001b[34mstimuli\u001b[m\u001b[m\n",
      "subject_number.txt\n",
      "tests_last_experiment.ipynb\n",
      "tests_last_experiment_accuracy_adjusted.ipynb\n",
      "trash.ipynb\n",
      "triplets.csv\n",
      "write_data.php\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "894b8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = Image.open('./stimuli/0_1.jpg') # This loads your image into memory\n",
    "img = img.convert('RGB') \n",
    "\n",
    "model.eval()\n",
    "\n",
    "image_x = transformer(img)\n",
    "\n",
    "prediction = model(image_x.view(-1, 3, 227, 227))\n",
    "# For a single image, the image must be reshaped into a batch\n",
    "# with size 1.\n",
    "# Get your prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee457985-8f16-4842-8ad8-9d6fd8423773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8225]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46714612-f816-487b-bd70-ce311fb3b270",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SaveFeatures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m ResMem(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m activation \u001b[38;5;241m=\u001b[39m \u001b[43mSaveFeatures\u001b[49m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mchildren())[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren())[\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Notice the numbers here, we have to select specific features a priori. We can't just record everything, because it would quickly fill up the computer's RAM.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# In the SaveFeatures object, we are selecting a specific *Residual Block*. The blocks are organized into \"sections\". Above we select section 5, block 4. Note that the children selector can also select non-convolutional modules within ResMem\u001b[39;00m\n\u001b[1;32m     12\u001b[0m memorability \u001b[38;5;241m=\u001b[39m model(transformer(randim)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m227\u001b[39m, \u001b[38;5;241m227\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SaveFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "# Generating a random image for demonstration purposes. You would want to use the image you're analyzing.\n",
    "randim = np.random.randint(0, 256, (256, 256, 3))\n",
    "randim = Image.fromarray(randim, mode='RGB')\n",
    "\n",
    "# Set up ResMem for prediction\n",
    "model = ResMem(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "activation = SaveFeatures(list(list(model.features.children())[5].children())[4])\n",
    "# Notice the numbers here, we have to select specific features a priori. We can't just record everything, because it would quickly fill up the computer's RAM.\n",
    "# In the SaveFeatures object, we are selecting a specific *Residual Block*. The blocks are organized into \"sections\". Above we select section 5, block 4. Note that the children selector can also select non-convolutional modules within ResMem\n",
    "memorability = model(transformer(randim).view(-1, 3, 227, 227))\n",
    "# Now the activation object contains some data.\n",
    "# The first index here should always be zero, it's selecting a batch index. In the case where you're running this in batches, you'll want to slice across that index instead.\n",
    "# The second index selects a filter in that layer (the filter associated with a certain channel).\n",
    "activation.features[0, 1]\n",
    "# Even then, this filter activation is a 29x29 tensor, one for each time the filter was applied to some subimage.\n",
    "# For analyzing filter activations, you may simply average these individual neuron-level activations.\n",
    "activation.features[0, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cb5c5-df6b-4ce8-8349-6701035d5f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
