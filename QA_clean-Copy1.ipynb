{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1c2819-b1e6-413d-a94b-f518b720e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "import glob\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0922854f-6eb9-4b5b-a4a0-8f7d12c5c661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory and visualization style\n",
    "home_dir = op.abspath('./')\n",
    "data_files = glob.glob(op.join(home_dir, 'data', '*.csv'))\n",
    "sns.set_context('talk')\n",
    "\n",
    "column_params = {\n",
    "    'v2': {'n_cats': 5, 'labels': ['Least Similar', '', '  ', '   ', 'Most Similar']},\n",
    "    'it': {'n_cats': 5, 'labels': ['Least Similar', '', '  ', '   ', 'Most Similar']}\n",
    "}\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850a98b6-f70f-41e2-88a1-f5fd60fdf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unit_variance(df, col, unit, group=None, suffix=\"_within\"):\n",
    "\n",
    "    new_col = col + suffix\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    def demean(x):\n",
    "        return x - x.mean()\n",
    "\n",
    "    if group is None:\n",
    "        new = df_copy.groupby(unit)[col].transform(demean)\n",
    "        new += df_copy[col].mean()\n",
    "        df_copy[new_col] = new\n",
    "    else:\n",
    "        df_copy[new_col] = np.nan\n",
    "        for level, df_level in df_copy.groupby(group):\n",
    "            new = df_level.groupby(unit)[col].transform(demean)\n",
    "            new += df_level[col].mean()\n",
    "            df_copy.loc[new.index, new_col] = new\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def parse_dates(series):\n",
    "    \"\"\"Parse date strings, handling special cases like '24h'.\"\"\"\n",
    "    date_str = series.iloc[0]\n",
    "    if \"24h\" in date_str:\n",
    "        corrected_date_str = date_str.replace(\"24h\", \"00h\")\n",
    "        dt = pd.to_datetime(corrected_date_str, format='%Y-%m-%d_%Hh%M.%S.%f')\n",
    "        dt += timedelta(days=1)\n",
    "    else:\n",
    "        dt = pd.to_datetime(date_str, format='%Y-%m-%d_%Hh%M.%S.%f')\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537d6ab3-0c71-400d-b52c-c7e4d1a30f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_values_time(value):\n",
    "    \"\"\"Process mouse time values from string to list of numbers.\"\"\"\n",
    "    try:\n",
    "        if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "            cleaned = value.strip('[]').split(',')\n",
    "            if cleaned == ['']:  # Check if the list after stripping is empty\n",
    "                return None\n",
    "            numbers = [float(num) for num in cleaned if num.strip()]\n",
    "            return numbers\n",
    "        return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def process_values_click(value):\n",
    "    \"\"\"Process mouse click values from string to list of items.\"\"\"\n",
    "    try:\n",
    "        if isinstance(value, str) and value.startswith('[') and value.endswith(']'):\n",
    "            cleaned = value.strip('[]').split(',')\n",
    "            if cleaned == ['']:  # Check if the list after stripping is empty\n",
    "                return None\n",
    "            return [item for item in cleaned if item.strip()]\n",
    "        return None\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d9eb49-5a6b-4a9e-9b0b-1eb99a450be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99014 rows of data\n",
      "Total unique participants: 330\n"
     ]
    }
   ],
   "source": [
    "def df_creation(data_files, start_date, end_date):\n",
    "    \"\"\"Create a dataframe from the data files within a date range.\"\"\"\n",
    "    processed_dfs = []\n",
    "    \n",
    "    # Loop through files and try to read them\n",
    "    for file_path in data_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            temp_df['filename'] = file_path\n",
    "            processed_dfs.append(temp_df)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error with {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Process the loaded dataframes\n",
    "    if processed_dfs:\n",
    "        # Concatenate all dataframes\n",
    "        df = pd.concat(processed_dfs, ignore_index=True)\n",
    "        \n",
    "        # Convert date column and filter by date range\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d_%Hh%M.%S.%f', errors='coerce')\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "        df = df[(df['date'] >= pd.to_datetime(start_date)) & (df['date'] <= pd.to_datetime(end_date))]\n",
    "        \n",
    "        # Filter rows with non-null V2_diff values\n",
    "        df = df.loc[df['V2_diff'].notnull()].reset_index(drop=True)\n",
    "        \n",
    "        # Convert reliability to float and create Retrocue Reliability\n",
    "        df['reliability'] = df['reliability'].astype(float)\n",
    "        df['Retrocue Reliability'] = np.where(df['reliability'] > 0.75, 'high', 'low')\n",
    "        \n",
    "        # Get numeric columns for later use\n",
    "        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "        \n",
    "        return df, numeric_columns\n",
    "    else:\n",
    "        # Return empty DataFrame if no files were processed\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "# Load data from both pilot periods\n",
    "df1, numeric_columns1 = df_creation(data_files, '2024-10-08', '2024-10-30')  # pilot5\n",
    "df2, numeric_columns2 = df_creation(data_files, '2024-11-22', '2025-01-30')  # pilot6\n",
    "# df = df2\n",
    "# Concatenate the data from both pilots\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "numeric_columns = numeric_columns1  # Use columns from first dataset\n",
    "\n",
    "print(f\"Loaded {len(df)} rows of data\")\n",
    "# print(f\"Data from pilot 1: {len(df1)} rows\")\n",
    "# print(f\"Data from pilot 2: {len(df2)} rows\")\n",
    "print(f\"Total unique participants: {df['participant'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad43e8d-e34b-4e5f-8df0-65d92f29cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Earliest file for each participant:\n",
      "     participant                                  earliest_filename\n",
      "0         156250  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "1         157210  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "2         152017  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "3         148645  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "4         156991  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "..           ...                                                ...\n",
      "325       145480  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "326       154459  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "327       153394  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "328       136771  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "329       167431  /Users/lana/Desktop/psychoPyExperiments/wm_dee...\n",
      "\n",
      "[330 rows x 2 columns]\n",
      "\n",
      "Cleaned DataFrame (only earliest file entries per participant):\n"
     ]
    }
   ],
   "source": [
    "first_files = df.groupby('participant', sort=False)['filename'].first().reset_index()\n",
    "first_files = first_files.rename(columns={'filename': 'earliest_filename'})\n",
    "print(\"\\nEarliest file for each participant:\")\n",
    "print(first_files)\n",
    "\n",
    "df = pd.merge(df, first_files, on='participant', how='left')\n",
    "\n",
    "df = df[df['filename'] == df['earliest_filename']].copy()\n",
    "\n",
    "df.drop('earliest_filename', axis=1, inplace=True)\n",
    "\n",
    "print(\"\\nCleaned DataFrame (only earliest file entries per participant):\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e114ceaa-835b-4aea-90f9-a4f888b7e1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008ee4db-cecb-4bab-b094-a417f6f7db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['taskPhase'] == 'mainTask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f680f62-ca29-4db4-b045-d81490c79076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 participants who appear in multiple files:\n"
     ]
    }
   ],
   "source": [
    "def identify_participants_in_multiple_files(dataframe):\n",
    "    # Count unique files per participant\n",
    "    participant_file_count = dataframe.groupby('participant')['filename'].nunique().reset_index()\n",
    "    \n",
    "    # Filter to only keep participants who appear in more than one file\n",
    "    multi_file_participants = participant_file_count[participant_file_count['filename'] > 1]\n",
    "    \n",
    "    print(f\"Found {len(multi_file_participants)} participants who appear in multiple files:\")\n",
    "    if len(multi_file_participants) > 0:\n",
    "        print(multi_file_participants)\n",
    "    \n",
    "    return multi_file_participants\n",
    "\n",
    "# Add this after your other cleaning steps\n",
    "multi_file_participants = identify_participants_in_multiple_files(df)\n",
    "\n",
    "# If you want to get the actual data for these participants:\n",
    "multi_file_participant_data = df[df['participant'].isin(multi_file_participants['participant'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed4a4d4-4f27-41bf-a236-d20af47313b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73936535-085f-4e33-b585-398ebf1e1347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added trial numbers and batch information. Max trial number: 300\n"
     ]
    }
   ],
   "source": [
    "def add_trial_info(df, participant_col, trials_per_batch=30):\n",
    "    \"\"\"Add trial number and batch information to the dataframe.\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Trial_Number'] = df_copy.groupby(participant_col).cumcount() + 1\n",
    "    df_copy['Trial_Batch'] = ((df_copy['Trial_Number'] - 1) // trials_per_batch) + 1\n",
    "    return df_copy\n",
    "\n",
    "df = add_trial_info(df, participant_col='participant')\n",
    "\n",
    "# Add condition batch information\n",
    "# Extract batch info from trial 182\n",
    "df_trial_182 = df[df['trial'] == 182][['participant', 'trial', 'cond_file', 'root', 'IT_diff']].copy()\n",
    "df_trial_182['conditions_batch'] = df_trial_182.groupby(['cond_file', 'root', 'IT_diff']).ngroup() + 1\n",
    "\n",
    "# Count participants per batch\n",
    "batch_info = df_trial_182.groupby('conditions_batch').agg(\n",
    "    participants_count=('participant', 'nunique'),\n",
    "    participants_list=('participant', 'unique')\n",
    ").reset_index()\n",
    "\n",
    "# First merge the conditions_batch to df_trial_182_with_counts\n",
    "df_trial_182_with_counts = df_trial_182.merge(\n",
    "    batch_info[['conditions_batch', 'participants_count', 'participants_list']],\n",
    "    on='conditions_batch',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Then merge this back to the main dataframe\n",
    "df = df.merge(\n",
    "    df_trial_182_with_counts[['participant', 'conditions_batch', 'participants_count', 'participants_list']],\n",
    "    on='participant',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Added trial numbers and batch information. Max trial number: {df['Trial_Number'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c51b14-c617-4b8d-8f5c-8baa28718a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7c2fb2-831d-4535-87bb-172ff9ebef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_differences(df):\n",
    "    \"\"\"Calculate differences between conditions for IT and V2 signals.\"\"\"\n",
    "    # Create a copy to avoid fragmentation warnings\n",
    "    df_diff = df.copy()\n",
    "    \n",
    "    # Create all new columns in a single dictionary\n",
    "    new_columns = {}\n",
    "    \n",
    "    # Attended and unattended conditions\n",
    "    new_columns['it_sim_dis_attend'] = np.where(df['attend'] == 'img1', df['IT_root_im1'], df['IT_root_im2'])\n",
    "    new_columns['v2_sim_dis_attend'] = np.where(df['attend'] == 'img1', df['V2_root_im1'], df['V2_root_im2'])\n",
    "    new_columns['it_sim_dis_test'] = np.where(df['test_item'] == 'img1', df['IT_root_im1'], df['IT_root_im2'])\n",
    "    new_columns['v2_sim_dis_test'] = np.where(df['test_item'] == 'img1', df['V2_root_im1'], df['V2_root_im2'])\n",
    "    new_columns['it_sim_dis_unattend'] = np.where(df['attend'] != 'img1', df['IT_root_im1'], df['IT_root_im2'])\n",
    "    new_columns['v2_sim_dis_unattend'] = np.where(df['attend'] != 'img1', df['V2_root_im1'], df['V2_root_im2'])\n",
    "    new_columns['it_sim_dis_untest'] = np.where(df['test_item'] != 'img1', df['IT_root_im1'], df['IT_root_im2'])\n",
    "    new_columns['v2_sim_dis_untest'] = np.where(df['test_item'] != 'img1', df['V2_root_im1'], df['V2_root_im2'])\n",
    "    \n",
    "    # Calculate differences\n",
    "    new_columns['it_sim_dis_diff'] = np.where(\n",
    "        df['attend'] == 'img1', \n",
    "        df['IT_root_im1'] - df['IT_root_im2'], \n",
    "        df['IT_root_im2'] - df['IT_root_im1']\n",
    "    )\n",
    "    new_columns['v2_sim_dis_diff'] = np.where(\n",
    "        df['attend'] == 'img1', \n",
    "        df['V2_root_im1'] - df['V2_root_im2'],\n",
    "        df['V2_root_im2'] - df['V2_root_im1']\n",
    "    )\n",
    "    \n",
    "    new_columns['it_sim_dis_diff_test'] = np.where(\n",
    "        df['test_item'] == 'img1', \n",
    "        df['IT_root_im1'] - df['IT_root_im2'], \n",
    "        df['IT_root_im2'] - df['IT_root_im1']\n",
    "    )\n",
    "    new_columns['v2_sim_dis_diff_test'] = np.where(\n",
    "        df['test_item'] == 'img1', \n",
    "        df['V2_root_im1'] - df['V2_root_im2'], \n",
    "        df['V2_root_im2'] - df['V2_root_im1']\n",
    "    )\n",
    "    \n",
    "    # Copy image similarity values\n",
    "    new_columns['it_im1_im2'] = df['IT_im1_im2']\n",
    "    new_columns['v2_im1_im2'] = df['V2_im1_im2']\n",
    "    \n",
    "    # Add all new columns at once to avoid fragmentation\n",
    "    for col_name, values in new_columns.items():\n",
    "        df_diff[col_name] = values\n",
    "    \n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dd7c4b2-84d3-48eb-b68a-791a3e2ac68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated signal differences and added preferences\n"
     ]
    }
   ],
   "source": [
    "# Continue the calculate_differences function\n",
    "def add_preferences(df):\n",
    "    \"\"\"Add preference indicators based on similarity values.\"\"\"\n",
    "    df_pref = df.copy()\n",
    "    \n",
    "    # Determine convergence and preferences\n",
    "    df_pref['v2_converges'] = np.where(\n",
    "        (df_pref['it_sim_dis_diff'] > 0) & (df_pref['v2_sim_dis_diff'] > 0) | \n",
    "        (df_pref['it_sim_dis_diff'] < 0) & (df_pref['v2_sim_dis_diff'] < 0), \n",
    "        'V2/IT agree', 'V2/IT disagree'\n",
    "    )\n",
    "    \n",
    "    # Determine preferences\n",
    "    df_pref['v2_prefers'] = np.where(df_pref['v2_sim_dis_diff'] > 0, 'Prioritized', 'Deprioritized')\n",
    "    df_pref['it_prefers'] = np.where(df_pref['it_sim_dis_diff'] > 0, 'Prioritized', 'Deprioritized')\n",
    "    df_pref['v2_prefers_test'] = np.where(df_pref['v2_sim_dis_diff_test'] > 0, 'Tested', 'Untested')\n",
    "    df_pref['it_prefers_test'] = np.where(df_pref['it_sim_dis_diff_test'] > 0, 'Tested', 'Untested')\n",
    "    \n",
    "    # Add preference columns with better names\n",
    "    df_pref['Distractor V2 Similarity Preference Tested'] = df_pref['v2_prefers_test']\n",
    "    df_pref['Distractor IT Similarity Preference Tested'] = df_pref['it_prefers_test']\n",
    "    df_pref['Distractor V2 Similarity Preference'] = df_pref['v2_prefers']\n",
    "    df_pref['Distractor IT Similarity Preference'] = df_pref['it_prefers']\n",
    "    \n",
    "    # Create binned versions of differences\n",
    "    df_pref['IT_diff_binned'] = pd.qcut(df_pref['it_sim_dis_diff'], 5, duplicates='drop')\n",
    "    df_pref['V2_diff_binned'] = pd.qcut(df_pref['v2_sim_dis_diff'], 5, duplicates='drop')\n",
    "    df_pref['IT_diff_binned_test'] = pd.qcut(df_pref['it_sim_dis_diff_test'], 5, duplicates='drop')\n",
    "    df_pref['V2_diff_binned_test'] = pd.qcut(df_pref['v2_sim_dis_diff_test'], 5, duplicates='drop')\n",
    "    \n",
    "    return df_pref\n",
    "\n",
    "# Apply signal difference calculations\n",
    "df = calculate_differences(df)\n",
    "df = add_preferences(df)\n",
    "print(f\"Calculated signal differences and added preferences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd89ffd-a6ae-4961-a08b-b30c95af08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b09d2b03-bdc9-43c1-9b5f-435a11433fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added categorized columns and user interface labels\n"
     ]
    }
   ],
   "source": [
    "def categorize_columns(df, column_params):\n",
    "    \"\"\"Categorize specified columns into discrete categories based on quantiles.\"\"\"\n",
    "    df_cat = df.copy()\n",
    "    \n",
    "    # Create all categorized columns at once\n",
    "    cat_columns = {}\n",
    "    \n",
    "    for label in [\n",
    "        'it_sim_dis_attend', 'v2_sim_dis_attend', 'it_sim_dis_unattend', 'v2_sim_dis_unattend',\n",
    "        'it_sim_dis_diff', 'v2_sim_dis_diff', 'it_im1_im2', 'v2_im1_im2',\n",
    "        'it_sim_dis_test', 'v2_sim_dis_test', 'it_sim_dis_untest', 'v2_sim_dis_untest',\n",
    "        'it_sim_dis_diff_test', 'v2_sim_dis_diff_test'\n",
    "    ]:\n",
    "        # Determine the column prefix\n",
    "        column_prefix = 'v2' if 'v2' in label else 'it'\n",
    "        \n",
    "        # Get parameters\n",
    "        n_cats = column_params[column_prefix]['n_cats']\n",
    "        labels = column_params[column_prefix]['labels']\n",
    "        \n",
    "        # Create categorized column\n",
    "        cat_columns[label + '_cat'] = pd.qcut(\n",
    "            df_cat[label], \n",
    "            q=n_cats, \n",
    "            labels=labels, \n",
    "            duplicates='drop'\n",
    "        )\n",
    "    \n",
    "    # Add all categorized columns at once\n",
    "    for col_name, values in cat_columns.items():\n",
    "        df_cat[col_name] = values\n",
    "    \n",
    "    return df_cat\n",
    "\n",
    "def validity_assignment(df):\n",
    "    \"\"\"Create 'Tested Item' column based on validity.\"\"\"\n",
    "    df_validity = df.copy()\n",
    "    df_validity['Tested Item'] = np.where(df_validity['validity'] == 'valid', 'prioritized', 'deprioritized')\n",
    "    return df_validity\n",
    "\n",
    "def df_column_addition(df):\n",
    "    \"\"\"Add user-friendly column names for plots and analyses.\"\"\"\n",
    "    df_add = df.copy()\n",
    "    \n",
    "    # Create a dictionary of all new columns\n",
    "    new_columns = {\n",
    "        'V2 Distractor Similarity\\nto Prioritized Item': df['v2_sim_dis_attend_cat'],\n",
    "        'IT Distractor Similarity\\nto Prioritized Item': df['it_sim_dis_attend_cat'],\n",
    "        'V2 Distractor Similarity\\nto Deprioritized Item': df['v2_sim_dis_unattend_cat'],\n",
    "        'IT Distractor Similarity\\nto Deprioritized Item': df['it_sim_dis_unattend_cat'],\n",
    "        'Prioritized - Deprioritized IT Distractor Similarity': df['it_sim_dis_diff_cat'],\n",
    "        'Prioritized - Deprioritized V2 Distractor Similarity': df['v2_sim_dis_diff_cat'],\n",
    "        'V2 Distractor Similarity\\nto Tested Item': df['v2_sim_dis_test_cat'],\n",
    "        'IT Distractor Similarity\\nto Tested Item': df['it_sim_dis_test_cat'],\n",
    "        'V2 Distractor Similarity\\nto Untested Item': df['v2_sim_dis_untest_cat'],\n",
    "        'IT Distractor Similarity\\nto Untested Item': df['it_sim_dis_untest_cat'],\n",
    "        'Tested - Untested IT Distractor Similarity': df['it_sim_dis_diff_test_cat'],\n",
    "        'Tested - Untested V2 Distractor Similarity': df['v2_sim_dis_diff_test_cat'],\n",
    "        'Prioritized - Deprioritized V2 Distractor Similarity Ranges': df['V2_diff_binned'],\n",
    "        'Prioritized - Deprioritized IT Distractor Similarity Ranges': df['IT_diff_binned'],\n",
    "        'Tested - Untested V2 Distractor Similarity Ranges': df['V2_diff_binned_test'],\n",
    "        'Tested - Untested IT Distractor Similarity Ranges': df['IT_diff_binned_test'],\n",
    "        'tested_item': df['Tested Item'],\n",
    "        'ret_rel': df['Retrocue Reliability'],\n",
    "        'validity_binary': df['Tested Item'].apply(lambda x: 1 if x == 'prioritized' else 0),\n",
    "        'reliability_binary': df['Retrocue Reliability'].apply(lambda x: 1 if x == 'high' else 0)\n",
    "    }\n",
    "    \n",
    "    # Add all new columns at once\n",
    "    for col_name, values in new_columns.items():\n",
    "        df_add[col_name] = values\n",
    "    \n",
    "    return df_add\n",
    "\n",
    "# Apply processing\n",
    "df = validity_assignment(df)\n",
    "df = categorize_columns(df, column_params)\n",
    "df = df_column_addition(df)\n",
    "\n",
    "print(f\"Added categorized columns and user interface labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bdb5d38-6de9-43bd-a2be-c47d44b47721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_empty_clicked_names(df):\n",
    "    df['none_clicked'] = df['mouse.clicked_name'].apply(\n",
    "        lambda x: (isinstance(x, list) and len(x) == 0) or \n",
    "                  (isinstance(x, str) and x.strip() == '[]')\n",
    "    )\n",
    "    # Group by participant and sum the Booleans, where True counts as 1.\n",
    "    result = df.groupby('participant')['none_clicked'].sum().reset_index()\n",
    "    \n",
    "    # Rename the column to something more descriptive.\n",
    "    result.rename(columns={'none_clicked': 'none_clicked_count'}, inplace=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bd7bfa8-50a4-42f4-8f51-4c77c437062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Participant statistics (total entries, empty count, empty ratio):\n",
      "\n",
      "Cleaned DataFrame (removed participants with >2/3 empty entries):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_participants_by_empty_clicked(df, threshold=1/3):\n",
    "   \n",
    "    df['empty_clicked'] = df['mouse.clicked_name'].apply(\n",
    "        lambda x: (isinstance(x, list) and len(x) == 0) or (isinstance(x, str) and x.strip() == '[]')\n",
    "    )\n",
    "    \n",
    "    participant_stats = df.groupby('participant').agg(\n",
    "        total_entries=('participant', 'size'),\n",
    "        empty_count=('empty_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    participant_stats['empty_ratio'] = participant_stats['empty_count'] / participant_stats['total_entries']\n",
    "    \n",
    "    participants_to_drop = participant_stats.loc[participant_stats['empty_ratio'] > threshold, 'participant']\n",
    "    \n",
    "    df_cleaned = df[~df['participant'].isin(participants_to_drop)].copy()\n",
    "    \n",
    "    return df_cleaned, participant_stats\n",
    "\n",
    "cleaned_df, stats_df = clean_participants_by_empty_clicked(df, 0.33)\n",
    "    \n",
    "print(\"\\nParticipant statistics (total entries, empty count, empty ratio):\")\n",
    "# print(stats_df)\n",
    "\n",
    "print(\"\\nCleaned DataFrame (removed participants with >2/3 empty entries):\")\n",
    "# print(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2126627-bfcb-4c72-b627-c9ae16a81092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(cleaned_df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ee0535-e715-4003-ab00-184c57ce4e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>total_entries</th>\n",
       "      <th>empty_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [participant, total_entries, empty_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_stats = cleaned_df.groupby('participant').agg(\n",
    "        total_entries=('participant', 'size'),\n",
    "        empty_count=('empty_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "participant_stats[participant_stats['participant'] == 159733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cfe31c9-a467-4655-a78c-bd6eaf6a8e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cleaned_df\n",
    "len(set(cleaned_df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce1e366e-9d92-41db-9789-0261897098ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed mouse data\n"
     ]
    }
   ],
   "source": [
    "def process_mouse_data(df):\n",
    "    \"\"\"Process mouse data to extract time and click information.\"\"\"\n",
    "    df_mouse = df.copy()\n",
    "    \n",
    "    # Process mouse time and click data\n",
    "    df_mouse['processed_mouse.time'] = df['mouse.time'].apply(process_values_time)\n",
    "    df_mouse['processed_mouse.click'] = df['mouse.clicked_name'].apply(process_values_click)\n",
    "    \n",
    "    # Extract length, first and last values\n",
    "    df_mouse['mouse.time_length'] = df_mouse['processed_mouse.time'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else 0\n",
    "    )\n",
    "    df_mouse['mouse.time_first'] = df_mouse['processed_mouse.time'].apply(\n",
    "        lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    df_mouse['mouse.time_last'] = df_mouse['processed_mouse.time'].apply(\n",
    "        lambda x: x[-1] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    df_mouse['mouse.clicked_name_first'] = df_mouse['processed_mouse.click'].apply(\n",
    "        lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    df_mouse['mouse.clicked_name_last'] = df_mouse['processed_mouse.click'].apply(\n",
    "        lambda x: x[-1] if isinstance(x, list) and len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_mouse\n",
    "\n",
    "# Process mouse data\n",
    "df = process_mouse_data(df)\n",
    "print(f\"Processed mouse data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8143340-3f35-4b5a-b687-46cb234e923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93986e79-3a67-4a7c-8bda-64ef7cf825f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_from_nonresp(df):\n",
    "    \"\"\"Filter out rows with missing mouse response data.\"\"\"\n",
    "    # Define a filter to check if mouse data exists\n",
    "    def filter_ranges(numbers):\n",
    "        if numbers is None:\n",
    "            return False\n",
    "        return all(num for num in numbers)\n",
    "    \n",
    "    # Apply filters for mouse time and click\n",
    "    filtered_df = df[df['processed_mouse.time'].apply(filter_ranges)]\n",
    "    filtered_df = filtered_df[filtered_df['processed_mouse.click'].apply(filter_ranges)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def df_with_threshold(df, numeric_columns, threshold=0.4):\n",
    "    \"\"\"Filter to include only participants with accuracy above threshold.\"\"\"\n",
    "    # Calculate mean accuracy by participant\n",
    "    sub_df = df[numeric_columns].groupby('participant').mean().reset_index()\n",
    "    \n",
    "    # Find participants above threshold\n",
    "    above_threshold_subs = sub_df.loc[sub_df['resp_correct'] >= threshold, 'participant']\n",
    "    \n",
    "    # Filter dataframe\n",
    "    df_filtered = df[df['participant'].isin(above_threshold_subs)]\n",
    "    \n",
    "    # Remove unit variance in accuracy\n",
    "    df_filtered = remove_unit_variance(df_filtered, 'resp_correct', 'participant')\n",
    "    df_filtered['Accuracy'] = df_filtered['resp_correct_within']\n",
    "    \n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd635397-9de1-4c7a-8663-02cbefe6051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant</th>\n",
       "      <th>filename</th>\n",
       "      <th>entry_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116851</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121021</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123262</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123790</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123985</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>170536</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>170665</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>171031</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>171253</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>171304</td>\n",
       "      <td>/Users/lana/Desktop/psychoPyExperiments/wm_dee...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant                                           filename  \\\n",
       "0         116851  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "1         121021  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "2         123262  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "3         123790  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "4         123985  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "..           ...                                                ...   \n",
       "292       170536  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "293       170665  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "294       171031  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "295       171253  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "296       171304  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
       "\n",
       "     entry_count  \n",
       "0            300  \n",
       "1            300  \n",
       "2            300  \n",
       "3            300  \n",
       "4            300  \n",
       "..           ...  \n",
       "292          300  \n",
       "293          300  \n",
       "294          300  \n",
       "295          300  \n",
       "296          300  \n",
       "\n",
       "[297 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove files with too few entries\n",
    "participant_file_counts_cleaned = df.groupby(['participant', 'filename']).agg(\n",
    "    entry_count=pd.NamedAgg(column='participant', aggfunc='size')\n",
    ").reset_index()\n",
    "participant_file_counts_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87e0735a-6db2-46ca-81db-493964f662c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f393953-2f75-4443-905c-51d402ccd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for participants with high rates of missing mouse data\n",
    "participant_file_counts = df.groupby(['participant', 'filename']).agg(\n",
    "    entry_count=pd.NamedAgg(column='participant', aggfunc='size'),\n",
    "    none_mouse=pd.NamedAgg(column='processed_mouse.time', aggfunc=lambda x: x.isna().sum())\n",
    ").reset_index()\n",
    "\n",
    "# Remove files with too many missing mouse responses\n",
    "files_with_high_none_mouse = participant_file_counts[participant_file_counts['none_mouse'] > 100]\n",
    "df = df[~df.set_index(['participant', 'filename']).index.isin(\n",
    "    files_with_high_none_mouse.set_index(['participant', 'filename']).index\n",
    ")]\n",
    "\n",
    "# Remove files with too few entries\n",
    "participant_file_counts_cleaned = df.groupby(['participant', 'filename']).agg(\n",
    "    entry_count=pd.NamedAgg(column='participant', aggfunc='size')\n",
    ").reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "692193f0-8b1b-4734-8be5-aa14f3ca2eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e04f707-7fd9-4153-bc7a-8ce473d43791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     participant                                           filename  \\\n",
      "101       149227  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
      "113       151639  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
      "194       164275  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
      "269       168529  /Users/lana/Desktop/psychoPyExperiments/wm_dee...   \n",
      "\n",
      "     entry_count  \n",
      "101           30  \n",
      "113          108  \n",
      "194           23  \n",
      "269          197  \n"
     ]
    }
   ],
   "source": [
    "files_with_low_entries = participant_file_counts_cleaned[participant_file_counts_cleaned['entry_count'] < 200]\n",
    "print(files_with_low_entries)\n",
    "df = df[~df.set_index(['participant', 'filename']).index.isin(\n",
    "    files_with_low_entries.set_index(['participant', 'filename']).index\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b21a2e04-0ae4-4f4a-9ca5-babfea88780a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9138936-3ab7-4415-a559-bef889fe8ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: count, dtype: int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant_counts = df['participant'].value_counts()\n",
    "participants_over_300 = participant_counts[participant_counts > 300]\n",
    "participants_over_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72323bfd-15c4-4865-92c4-bd6d3c72bfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b76c5c62-d096-4c12-9199-397f4cdd7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering non-responses: 83781 rows\n",
      "After accuracy threshold filtering: 78410 rows with 273 participants\n",
      "After filtering non-responses: 78410 rows\n",
      "After accuracy threshold filtering: 81000 rows with 270 participants\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning operations\n",
    "df_nonresp = clean_dataframe_from_nonresp(df)\n",
    "print(f\"After filtering non-responses: {len(df_nonresp)} rows\")\n",
    "\n",
    "\n",
    "df_nonresp = df_with_threshold(df_nonresp, numeric_columns, 0.4)\n",
    "print(f\"After accuracy threshold filtering: {len(df_nonresp)} rows with {df_nonresp['participant'].nunique()} participants\")\n",
    "\n",
    "# Convert to numeric and standardize within participants\n",
    "df['mouse.time_last'] = pd.to_numeric(df['mouse.time_last'], errors='coerce')\n",
    "df = remove_unit_variance(df, 'mouse.time_last', 'participant')\n",
    "df['analysis_rt'] = df['mouse.time_last_within']\n",
    "df['rt'] = df['mouse.time_last']\n",
    "\n",
    "\n",
    "\n",
    "print(f\"After filtering non-responses and thresholding: {len(df_nonresp)} rows\")\n",
    "\n",
    "\n",
    "df = df_with_threshold(df, numeric_columns, 0.4)\n",
    "print(f\"After accuracy threshold filtering: {len(df)} rows with {df['participant'].nunique()} participants\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "150785ba-6810-44b6-9b4f-1f25ad1f4699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['participant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77257f1-fe48-4e7d-995b-b02796103dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_z_score(df, list_of_variables):\n",
    "    df_z = df.copy()\n",
    "    new_columns = {}\n",
    "    \n",
    "    for l in list_of_variables:\n",
    "        # ----- Process squared version -----\n",
    "        # Square the variable and store in a new column name\n",
    "        squared_col = f\"{l}_sq\"\n",
    "        new_columns[squared_col] = df_z[l] ** 2\n",
    "        \n",
    "        # Standardize (z score) the squared variable\n",
    "        scaler_sq = StandardScaler()\n",
    "        squared_z = f\"{squared_col}_z\"\n",
    "        new_columns[squared_z] = scaler_sq.fit_transform(\n",
    "            pd.DataFrame(new_columns[squared_col])\n",
    "        ).flatten()\n",
    "        scaler_orig = StandardScaler()\n",
    "        original_z = f\"{l}_z\"\n",
    "        new_columns[original_z] = scaler_orig.fit_transform(\n",
    "            pd.DataFrame(df_z[l])\n",
    "        ).flatten()\n",
    "    \n",
    "    # Add all new columns at once\n",
    "    for col_name, values in new_columns.items():\n",
    "        df_z[col_name] = values\n",
    "        \n",
    "    return df_z\n",
    "\n",
    "# Apply standardization to similarity metrics\n",
    "sim_variables = [\n",
    "    'it_sim_dis_diff', 'v2_sim_dis_diff', \n",
    "    'it_sim_dis_attend', 'v2_sim_dis_attend',\n",
    "    'it_sim_dis_unattend', 'v2_sim_dis_unattend', \n",
    "    'it_sim_dis_test', 'v2_sim_dis_test',\n",
    "    'it_sim_dis_untest', 'v2_sim_dis_untest', \n",
    "    'it_sim_dis_diff_test', 'v2_sim_dis_diff_test'\n",
    "]\n",
    "\n",
    "# # Apply standardization\n",
    "# df = df_demean(df, sim_variables)\n",
    "df = df_z_score(df, sim_variables)\n",
    "\n",
    "\n",
    "scaler_v = StandardScaler()\n",
    "scaler_r = StandardScaler()\n",
    "\n",
    "# Update binary indicators to z-scored versions\n",
    "df['validity_binary'] = (df['validity'] == 'valid').astype(int)\n",
    "df['reliability_binary'] = (df['reliability'] > 0.7).astype(int)\n",
    "df['validity_binary_z'] = scaler_v.fit_transform(df[['validity_binary']])\n",
    "df['reliability_binary_z'] = scaler_r.fit_transform(df[['reliability_binary']])\n",
    "\n",
    "# Create additional UI columns with z-scored values\n",
    "ui_columns = {\n",
    "    'V2 Distractor Similarity to Tested Item': df['v2_sim_dis_test'],\n",
    "    'IT Distractor Similarity to Tested Item': df['it_sim_dis_test'],\n",
    "    'Tested - Untested V2 Distractor Similarity': df['v2_sim_dis_diff_test'],\n",
    "    'Tested - Untested IT Distractor Similarity': df['it_sim_dis_diff_test'],\n",
    "    'V2 Distractor Similarity\\nto Prioritized Item': df['v2_sim_dis_attend'],\n",
    "    'IT Distractor Similarity\\nto Prioritized Item': df['it_sim_dis_attend'],\n",
    "    'V2 Distractor Similarity\\nto Deprioritized Item': df['v2_sim_dis_unattend'],\n",
    "    'IT Distractor Similarity\\nto Deprioritized Item': df['it_sim_dis_unattend'],\n",
    "    'Prioritized - Deprioritized IT Distractor Similarity': df['it_sim_dis_diff'],\n",
    "    'Prioritized - Deprioritized V2 Distractor Similarity': df['v2_sim_dis_diff']\n",
    "}\n",
    "\n",
    "# Add UI columns at once\n",
    "for col_name, values in ui_columns.items():\n",
    "    df[col_name] = values\n",
    "\n",
    "print(f\"Added standardized variables and transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5f2b2-0428-42b2-85f7-fd0711299da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_z_sq_z(df, column_name):\n",
    "    \"\"\"\n",
    "    Create sign-preserving transformations for a column.\n",
    "    \"\"\"\n",
    "    df_flip = df.copy()\n",
    "    \n",
    "    # Dictionary to store all new columns\n",
    "    new_columns = {}\n",
    "    \n",
    "    # Create sign-preserved value\n",
    "    new_columns[column_name + '_sign'] = np.where(df[column_name] <= 0, -1, 1) * df[column_name]\n",
    "    scaler_z = StandardScaler()\n",
    "\n",
    "    # Z-score the sign-preserved value\n",
    "    new_columns[column_name + '_sign_z'] = scaler_z.fit_transform(\n",
    "        pd.DataFrame(new_columns[column_name + '_sign'])\n",
    "    ).flatten()\n",
    "    scaler_sq_z = StandardScaler()\n",
    "\n",
    "    # Square and z-score\n",
    "    new_columns[column_name + '_sign_sq'] = new_columns[column_name + '_sign_z'] ** 2\n",
    "    new_columns[column_name + '_sign_sq_z'] = scaler_sq_z.fit_transform(\n",
    "        pd.DataFrame(new_columns[column_name + '_sign_sq'])\n",
    "    ).flatten()\n",
    "    \n",
    "    # Add all new columns at once\n",
    "    for col_name, values in new_columns.items():\n",
    "        df_flip[col_name] = values\n",
    "    \n",
    "    return df_flip\n",
    "\n",
    "# Apply sign-preserving transformations\n",
    "for column in ['it_sim_dis_diff_test', 'v2_sim_dis_diff_test', 'it_sim_dis_test', 'v2_sim_dis_test']:\n",
    "    df = flip_z_sq_z(df, column)\n",
    "\n",
    "print(f\"Added sign-preserving transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0438a0-463b-408b-ac6b-32ab5de00b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positive/negative indicators and interaction terms\n",
    "pos_neg_columns = {\n",
    "    'it_pos_neg': np.where(df['it_sim_dis_diff_test'] <= 0, -1, 1),\n",
    "    'v2_pos_neg': np.where(df['v2_sim_dis_diff_test'] <= 0, -1, 1),\n",
    "    # 'it_pos_neg_abs': np.where(df['it_sim_dis_test'] <= 0, -1, 1),\n",
    "    # 'v2_pos_neg_abs': np.where(df['v2_sim_dis_test'] <= 0, -1, 1)\n",
    "}\n",
    "\n",
    "# Add positive/negative columns\n",
    "for col_name, values in pos_neg_columns.items():\n",
    "    df[col_name] = values\n",
    "    scaler_pn = StandardScaler()\n",
    "    df[col_name + '_z'] = scaler_pn.fit_transform(df[[col_name]])\n",
    "\n",
    "# Create interaction terms\n",
    "interaction_terms = {\n",
    "    'it_int_rel': df['it_pos_neg'] * df['it_sim_dis_diff_test_sign_z'],\n",
    "    'v2_int_rel': df['v2_pos_neg'] * df['v2_sim_dis_diff_test_sign_z'],\n",
    "    # 'it_int_abs': df['it_pos_neg_abs'] * df['it_sim_dis_test_sign_z'],\n",
    "    # 'v2_int_abs': df['v2_pos_neg_abs'] * df['v2_sim_dis_test_sign_z'],\n",
    "    'it_int_rel_sq': df['it_pos_neg'] * df['it_sim_dis_diff_test_sign_sq_z'],\n",
    "    'v2_int_rel_sq': df['v2_pos_neg'] * df['v2_sim_dis_diff_test_sign_sq_z'],\n",
    "    # 'it_int_abs_sq': df['it_pos_neg_abs'] * df['it_sim_dis_test_sign_sq_z'],\n",
    "    # 'v2_int_abs_sq': df['v2_pos_neg_abs'] * df['v2_sim_dis_test_sign_sq_z']\n",
    "}\n",
    "\n",
    "# Add interaction terms at once\n",
    "for col_name, values in interaction_terms.items():\n",
    "    df[col_name] = values\n",
    "\n",
    "print(f\"Added interaction terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998f6dc-4a6e-424f-958c-df5de66d70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonresponse_count = df['empty_clicked'].sum()\n",
    "total_trials = len(df)\n",
    "nonresponse_percentage = (nonresponse_count / total_trials) * 100\n",
    "\n",
    "print(f\"Number of nonresponse trials in final dataframe: {nonresponse_count}\")\n",
    "print(f\"Percentage of nonresponse trials: {nonresponse_percentage:.2f}%\")\n",
    "\n",
    "# Check distribution across participants\n",
    "participant_nonresponse = df.groupby('participant')['empty_clicked'].sum().reset_index()\n",
    "participant_nonresponse.columns = ['participant', 'nonresponse_count']\n",
    "participant_nonresponse['total_trials'] =df.groupby('participant').size().values\n",
    "participant_nonresponse['nonresponse_percentage'] = (participant_nonresponse['nonresponse_count'] / participant_nonresponse['total_trials']) * 100\n",
    "\n",
    "print(\"\\nSummary of nonresponses by participant:\")\n",
    "print(f\"Mean nonresponse percentage per participant: {participant_nonresponse['nonresponse_percentage'].mean():.2f}%\")\n",
    "print(f\"Max nonresponse percentage for any participant: {participant_nonresponse['nonresponse_percentage'].max():.2f}%\")\n",
    "print(f\"Number of participants with any nonresponses: {(participant_nonresponse['nonresponse_count'] > 0).sum()}\")\n",
    "\n",
    "# See participants with highest nonresponse rates\n",
    "print(\"\\nParticipants with highest nonresponse rates:\")\n",
    "print(participant_nonresponse.sort_values('nonresponse_percentage', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e109f-ff8e-48f8-8c65-ff328208b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d72784-2dae-4522-a8e7-3ac3cc1d38c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
